{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdb86eec"
      },
      "source": [
        "# Assignment 4: Multilingual BERT and Zero-Shot Transfer (10 Marks)\n",
        "\n",
        "## Due: 31 March 2022\n",
        "\n",
        "Welcome to the 4th and last assignment of the course. In this assignment we will learn how to fine-tune a multilingual BERT or mBERT model on a Natural Language Inference task [XNLI](https://arxiv.org/abs/1809.05053). We will fine-tune the model on English Training data and then evaluate the performance of the fine-tuned models on different languages demonstrating the zero-shot capabilities of mBERT. "
      ],
      "id": "fdb86eec"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cb7d9310",
        "outputId": "4d40a2dc-932e-4430-b77d-84970a590bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    data_dir = \"/content/gdrive/MyDrive/data/xnli\"\n",
        "except:\n",
        "    data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/source/Assignment4/data/xnli\""
      ],
      "id": "cb7d9310"
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8f1c0d05",
        "outputId": "71710cbb-b002-4d19-a891-1e5b3d4c08f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install transformers\n",
        "!pip install tqdm"
      ],
      "id": "8f1c0d05"
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "382573bd"
      },
      "outputs": [],
      "source": [
        "# We start by importing libraries that we will be making use of in the assignment.\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import tqdm\n"
      ],
      "id": "382573bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "222e4a45"
      },
      "source": [
        "## XNLI: Task Description\n",
        "\n",
        "XNLI is a multilingual benchmark for Natural Language Inference, that contains training data available in English which was obtained from the popular [MNLI](https://cims.nyu.edu/~sbowman/multinli/), and test and dev sets available for 15 different languages. In NLI, we are given two sentences, one is a premise and other an hypothesis, and the task is to predict whether the hypothesis is i) entialed in the premise, or ii) contradicts the premise, or iii) neutral to the premise. \n",
        "\n",
        "<img src=\"https://i.ibb.co/bd4P20K/nli-examples.jpg\" alt=\"nli-examples\" border=\"0\">\n",
        "\n",
        "This makes NLI a multi-class classification task where we want to predict the correct label out of the three possible classes. We start by loading the dataset into memory. The training set in XNLI is comparitively huge with around 400k examples, which can lead to higher training times. Hence for the purpose of this assignment we will work with a fraction of the full data i.e. ~40k examples"
      ],
      "id": "222e4a45"
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "40cf1fe5"
      },
      "outputs": [],
      "source": [
        "def load_xnli_dataset(lang, split = \"train\"):\n",
        "    filename = os.path.join(data_dir, f\"{split}-{lang}.tsv\")\n",
        "    sentence1s = []\n",
        "    sentence2s = []\n",
        "    labels = []\n",
        "    with open(filename) as f:\n",
        "        for i,line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            row = line.split(\"\\t\")\n",
        "            sentence1 = row[0]\n",
        "            sentence2 = row[1]\n",
        "            label = row[2].split(\"\\n\")[0]\n",
        "            sentence1s.append(sentence1)\n",
        "            sentence2s.append(sentence2)\n",
        "            labels.append((label))\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        \"premise\": sentence1s,\n",
        "        \"hypothesis\" : sentence2s,\n",
        "        \"label\" : labels\n",
        "    })"
      ],
      "id": "40cf1fe5"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "1d0b9f05",
        "outputId": "b5feee17-e1a9-4de8-9c33-56468fa932c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of examples in training data: 38000\n",
            "Number of examples in validation data: 2000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 premise  \\\n",
              "38965  is it uh i i believe is it sugar cane or i 'm ...   \n",
              "39006                       You 're a bit of a dream . '   \n",
              "37308                Is there anything cuter than that ?   \n",
              "37996  okay well um thanks for hearing me ramp i gues...   \n",
              "24109  It is this package that allows a manufacturer ...   \n",
              "\n",
              "                                              hypothesis          label  \n",
              "38965       I believe it 's sugar cane but I 'm not sure     entailment  \n",
              "39006                       You are reality in my head .  contradiction  \n",
              "37308                                  That is so ugly .  contradiction  \n",
              "37996  The conversation was pleasant even though I ra...        neutral  \n",
              "24109  The manufacturing plant was small but the prod...        neutral  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce6fae0a-3239-4a0f-a87d-07c99c41a816\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>38965</th>\n",
              "      <td>is it uh i i believe is it sugar cane or i 'm ...</td>\n",
              "      <td>I believe it 's sugar cane but I 'm not sure</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39006</th>\n",
              "      <td>You 're a bit of a dream . '</td>\n",
              "      <td>You are reality in my head .</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37308</th>\n",
              "      <td>Is there anything cuter than that ?</td>\n",
              "      <td>That is so ugly .</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37996</th>\n",
              "      <td>okay well um thanks for hearing me ramp i gues...</td>\n",
              "      <td>The conversation was pleasant even though I ra...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24109</th>\n",
              "      <td>It is this package that allows a manufacturer ...</td>\n",
              "      <td>The manufacturing plant was small but the prod...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce6fae0a-3239-4a0f-a87d-07c99c41a816')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce6fae0a-3239-4a0f-a87d-07c99c41a816 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce6fae0a-3239-4a0f-a87d-07c99c41a816');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "# Load Training data in english\n",
        "train_en_data = load_xnli_dataset(\"en\", \"train\")[:40000]\n",
        "\n",
        "#Like last assignment we will use split the training data to get some validation examples as well\n",
        "train_en_data, val_en_data = train_test_split(train_en_data, test_size=0.05)\n",
        "\n",
        "print(f\"Number of examples in training data: {len(train_en_data)}\")\n",
        "print(f\"Number of examples in validation data: {len(val_en_data)}\")\n",
        "\n",
        "train_en_data.head()"
      ],
      "id": "1d0b9f05"
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "96957733"
      },
      "outputs": [],
      "source": [
        "# Load Test data in other languages\n",
        "test_langs = [\"ar\", \"bg\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hi\", \"ru\", \"sw\", \"th\", \"tr\", \"ur\", \"vi\", \"zh\"]\n",
        "\n",
        "lang2test_df = {lang : load_xnli_dataset(lang, \"dev\") for lang in test_langs}"
      ],
      "id": "96957733"
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "d8bc71b7",
        "outputId": "839c61c4-765c-49ae-f1e7-4198aacbe340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Test examples: 2489\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             premise  \\\n",
              "0                       And he said, Mama, I'm home.   \n",
              "1                       And he said, Mama, I'm home.   \n",
              "2  I didn't know what I was going for or anything...   \n",
              "3  I didn't know what I was going for or anything...   \n",
              "4  I didn't know what I was going for or anything...   \n",
              "\n",
              "                                          hypothesis          label  \n",
              "0                              He didn't say a word.  contradiction  \n",
              "1                He told his mom he had gotten home.     entailment  \n",
              "2  I have never been to Washington so when I was ...        neutral  \n",
              "3  I knew exactly what I needed to do as I marche...  contradiction  \n",
              "4  I was not quite certain what I was going to do...     entailment  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91e5ab4e-5040-4e75-aea9-e25ac0085d6d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>And he said, Mama, I'm home.</td>\n",
              "      <td>He didn't say a word.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>And he said, Mama, I'm home.</td>\n",
              "      <td>He told his mom he had gotten home.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't know what I was going for or anything...</td>\n",
              "      <td>I have never been to Washington so when I was ...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I didn't know what I was going for or anything...</td>\n",
              "      <td>I knew exactly what I needed to do as I marche...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I didn't know what I was going for or anything...</td>\n",
              "      <td>I was not quite certain what I was going to do...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91e5ab4e-5040-4e75-aea9-e25ac0085d6d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91e5ab4e-5040-4e75-aea9-e25ac0085d6d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91e5ab4e-5040-4e75-aea9-e25ac0085d6d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "print(f\"Number of Test examples: {len(lang2test_df['en'])}\")\n",
        "lang2test_df[\"en\"].head()"
      ],
      "id": "d8bc71b7"
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "362ebef0",
        "outputId": "b7d9de8a-a091-4fe5-b77c-ec3081354044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ar test set:\n",
            "                                             premise  \\\n",
            "0                        وقال، ماما، لقد عدت للمنزل.   \n",
            "1                        وقال، ماما، لقد عدت للمنزل.   \n",
            "2  لم أعرف من أجل ماذا أنا ذاهب أو أي شىْ ، لذلك ...   \n",
            "3  لم أعرف من أجل ماذا أنا ذاهب أو أي شىْ ، لذلك ...   \n",
            "4  لم أعرف من أجل ماذا أنا ذاهب أو أي شىْ ، لذلك ...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                  لم ينطق ببنت شفة.  contradiction  \n",
            "1                        أخبر أمه أنه قد عاد للمنزل.     entailment  \n",
            "2  لم أذهب إلى واشنطن من قبل، لذا عندما تم تكليفي...        neutral  \n",
            "3  لقد عرفت بالضبط ما الذي احتجت أن أفعله عندما م...  contradiction  \n",
            "4  لم أكن متأكدًا مما سأفعله لذلك ذهبت إلى واشنطن...     entailment  \n",
            "***************************\n",
            "\n",
            "bg test set:\n",
            "                                             premise  \\\n",
            "0                      И той каза: Мамо, у дома съм.   \n",
            "1                      И той каза: Мамо, у дома съм.   \n",
            "2  Не знаех за какво отивам и въобще нищо, но тря...   \n",
            "3  Не знаех за какво отивам и въобще нищо, но тря...   \n",
            "4  Не знаех за какво отивам и въобще нищо, но тря...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                             Той не каза нито дума.  contradiction  \n",
            "1       Той каза на майка си, че се е прибрал вкъщи.     entailment  \n",
            "2  Никога не съм бил във Вашингтон и когато ме на...        neutral  \n",
            "3  Знаех точно какво да направя когато отивах към...  contradiction  \n",
            "4  Не бях съвсем сигурен какво ще направя, така ч...     entailment  \n",
            "***************************\n",
            "\n",
            "de test set:\n",
            "                                             premise  \\\n",
            "0            und er hat gesagt, Mama ich bin daheim.   \n",
            "1            und er hat gesagt, Mama ich bin daheim.   \n",
            "2  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
            "3  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
            "4  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                Er sagte kein Wort.  contradiction  \n",
            "1  Er sagte seiner Mutter, er sei nach Hause geko...     entailment  \n",
            "2  Ich war noch nie in Washington, deshalb habe i...        neutral  \n",
            "3  Ich wusste genau, was ich tun musste, als ich ...  contradiction  \n",
            "4  Ich war mir nicht ganz sicher was ich tun soll...     entailment  \n",
            "***************************\n",
            "\n",
            "el test set:\n",
            "                                             premise  \\\n",
            "0                  Και είπε, Μαμά, έφτασα στο σπίτι.   \n",
            "1                  Και είπε, Μαμά, έφτασα στο σπίτι.   \n",
            "2  Δεν ήξερα που πήγαινα ή κάτι τέτοιο, έτσι έπρε...   \n",
            "3  Δεν ήξερα που πήγαινα ή κάτι τέτοιο, έτσι έπρε...   \n",
            "4  Δεν ήξερα που πήγαινα ή κάτι τέτοιο, έτσι έπρε...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                Δεν είπε ούτε λέξη.  contradiction  \n",
            "1            Είπε στην μαμά του ότι είχε πάει σπίτι.     entailment  \n",
            "2  Ποτέ δεν πήγα στην Ουάσινγκτον, οπότε όταν με ...        neutral  \n",
            "3  Ήξερα ακριβώς τι χρειαζόμουν καθώς όδευα προς ...  contradiction  \n",
            "4  Δεν ήμουν αρκετά σίγουρος για το τι θα κάνω, έ...     entailment  \n",
            "***************************\n",
            "\n",
            "en test set:\n",
            "                                             premise  \\\n",
            "0                       And he said, Mama, I'm home.   \n",
            "1                       And he said, Mama, I'm home.   \n",
            "2  I didn't know what I was going for or anything...   \n",
            "3  I didn't know what I was going for or anything...   \n",
            "4  I didn't know what I was going for or anything...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                              He didn't say a word.  contradiction  \n",
            "1                He told his mom he had gotten home.     entailment  \n",
            "2  I have never been to Washington so when I was ...        neutral  \n",
            "3  I knew exactly what I needed to do as I marche...  contradiction  \n",
            "4  I was not quite certain what I was going to do...     entailment  \n",
            "***************************\n",
            "\n",
            "es test set:\n",
            "                                             premise  \\\n",
            "0                    Y él dijo: Mamá, estoy en casa.   \n",
            "1                    Y él dijo: Mamá, estoy en casa.   \n",
            "2  No sabía para qué iba ni nada, así que iba a i...   \n",
            "3  No sabía para qué iba ni nada, así que iba a i...   \n",
            "4  No sabía para qué iba ni nada, así que iba a i...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                            Él no dijo una palabra.  contradiction  \n",
            "1       Le dijo a su madre que había llegado a casa.     entailment  \n",
            "2  Nunca he estado en Washington, así que cuando ...        neutral  \n",
            "3  Sabía exactamente lo que tenía que hacer mient...  contradiction  \n",
            "4  No estaba muy seguro de lo que iba a hacer, as...     entailment  \n",
            "***************************\n",
            "\n",
            "fr test set:\n",
            "                                             premise  \\\n",
            "0           Et il a dit, maman, je suis à la maison.   \n",
            "1           Et il a dit, maman, je suis à la maison.   \n",
            "2  Je ne savais pas dans quoi je me lançais, donc...   \n",
            "3  Je ne savais pas dans quoi je me lançais, donc...   \n",
            "4  Je ne savais pas dans quoi je me lançais, donc...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                             Il n'a pas dit un mot.  contradiction  \n",
            "1             Il a dit à sa mère qu'il était rentré.     entailment  \n",
            "2  Je ne suis jamais allé à Washington et donc qu...        neutral  \n",
            "3  Je savais exactement ce que j'avais à faire qu...  contradiction  \n",
            "4  Je n'étais pas tout à fait certain de ce que j...     entailment  \n",
            "***************************\n",
            "\n",
            "hi test set:\n",
            "                                             premise  \\\n",
            "0                  और उसने कहा, माँ, मैं घर आया हूं।   \n",
            "1                  और उसने कहा, माँ, मैं घर आया हूं।   \n",
            "2  मुझे नहीं पता था कि मैं क्या कर रहा था या कुछ ...   \n",
            "3  मुझे नहीं पता था कि मैं क्या कर रहा था या कुछ ...   \n",
            "4  मुझे नहीं पता था कि मैं क्या कर रहा था या कुछ ...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                             उसने एक शब्द नहीं कहा।  contradiction  \n",
            "1          Uski maa ne bataya ki wo ghar pahuch gaya     entailment  \n",
            "2  मैं कभी वाशिंगटन नहीं गया, इसलिए जब मुझे काम स...        neutral  \n",
            "3  मैं बिल्कुल जानता था कि मुझे वाशिंगटन जाने के ...  contradiction  \n",
            "4  Mujhe kuch nahi pata tha kya karna hai, to mai...     entailment  \n",
            "***************************\n",
            "\n",
            "ru test set:\n",
            "                                             premise  \\\n",
            "0                         И он сказал: Мама, я дома.   \n",
            "1                         И он сказал: Мама, я дома.   \n",
            "2  Я не знал, что мне предстояло сделать и все та...   \n",
            "3  Я не знал, что мне предстояло сделать и все та...   \n",
            "4  Я не знал, что мне предстояло сделать и все та...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                           Он не произнес ни слова.  contradiction  \n",
            "1          Он сказал матери, что уже добрался домой.     entailment  \n",
            "2  Я раньше не был в Вашингтоне, поэтому, получив...        neutral  \n",
            "3  Я точно знал, что мне нужно сделать, когда вхо...  contradiction  \n",
            "4  Я не знал, что мне делать, поэтому я отправилс...     entailment  \n",
            "***************************\n",
            "\n",
            "sw test set:\n",
            "                                             premise  \\\n",
            "0                 Naye akasema, Mama, niko nyumbani.   \n",
            "1                 Naye akasema, Mama, niko nyumbani.   \n",
            "2  Sikujua nini nilichoendea au kitu chochote, hi...   \n",
            "3  Sikujua nini nilichoendea au kitu chochote, hi...   \n",
            "4  Sikujua nini nilichoendea au kitu chochote, hi...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                 Hakusema chochote.  contradiction  \n",
            "1     Alimwambia mama yake alikuwa amefika nyumbani.     entailment  \n",
            "2  Sijawahi kwenda Washington hivyo wakati nilipo...        neutral  \n",
            "3  Nilijua hasa kile nilichohitaji kufanya  nilip...  contradiction  \n",
            "4  Sikuwa na hakika kabisa nilichokuwa nikienda k...     entailment  \n",
            "***************************\n",
            "\n",
            "th test set:\n",
            "                                             premise  \\\n",
            "0                    และเขาพูดว่า, ม่าม๊า ผมอยู่บ้าน   \n",
            "1                    และเขาพูดว่า, ม่าม๊า ผมอยู่บ้าน   \n",
            "2  ฉันไม่รู้ว่าฉันไปเพื่ออะไรหรือเพื่อสิ่งใด ดังน...   \n",
            "3  ฉันไม่รู้ว่าฉันไปเพื่ออะไรหรือเพื่อสิ่งใด ดังน...   \n",
            "4  ฉันไม่รู้ว่าฉันไปเพื่ออะไรหรือเพื่อสิ่งใด ดังน...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                  เขาไม่ได้พูดสักคำ  contradiction  \n",
            "1                    เขาบอกเเม่เขาว่าเขาถึงบ้านเเล้ว     entailment  \n",
            "2  ฉันไม่เคยไป กรุงวอชิงตันมาก่อนเลย เพราะเช่นนั้...        neutral  \n",
            "3  ฉันรู้อยู่เเล้วว่าฉันจะต้องทำยังไงจะปรับตัวกับ...  contradiction  \n",
            "4  ฉันไม่ค่อยมั่นใจว่าฉันจะทำอะไร ดังนั้นฉันจึงไป...     entailment  \n",
            "***************************\n",
            "\n",
            "tr test set:\n",
            "                                             premise  \\\n",
            "0                             Ve Anne, evdeyim dedi.   \n",
            "1                             Ve Anne, evdeyim dedi.   \n",
            "2  Ne için gittiğimi falan bilmiyordum, Washingto...   \n",
            "3  Ne için gittiğimi falan bilmiyordum, Washingto...   \n",
            "4  Ne için gittiğimi falan bilmiyordum, Washingto...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                              Bir kelime söylemedi.  contradiction  \n",
            "1                    Annesine eve gittiğini söyledi.     entailment  \n",
            "2  Washington'a hiç gitmedim, bu yüzden oraya ata...        neutral  \n",
            "3  Washington'a yürürken ne yapmam gerektiğini ta...  contradiction  \n",
            "4  Ne yapacağımdan çok emin değildim, o yüzden ra...     entailment  \n",
            "***************************\n",
            "\n",
            "ur test set:\n",
            "                                             premise  \\\n",
            "0              اور اس نے کہا امّی، میں گھر آگیا ہوں۔   \n",
            "1              اور اس نے کہا امّی، میں گھر آگیا ہوں۔   \n",
            "2  مجھے نہیں معلوم تھا کہ میں کیا کرنے جا رہا تھا...   \n",
            "3  مجھے نہیں معلوم تھا کہ میں کیا کرنے جا رہا تھا...   \n",
            "4  مجھے نہیں معلوم تھا کہ میں کیا کرنے جا رہا تھا...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                               وں ا یک لفز نھی بولا  contradiction  \n",
            "1               اسنی اپنی امی کو بتایا کے وں گھر ھیں     entailment  \n",
            "2  میں کبھی واشنگٹن نہیں گیا/گئی چناں چہ جب میرا ...        neutral  \n",
            "3  میں جانتا تھا کہ واشنگٹن کی طرف روانہ ہونے کے ...  contradiction  \n",
            "4  میں بالکل اس بات کا پتا نہی تھا کہ میں کیا کرن...     entailment  \n",
            "***************************\n",
            "\n",
            "vi test set:\n",
            "                                             premise  \\\n",
            "0                  Và anh ấy nói, Mẹ, con đã về nhà.   \n",
            "1                  Và anh ấy nói, Mẹ, con đã về nhà.   \n",
            "2  Tôi đã không biết mình đang hướng tới mục đích...   \n",
            "3  Tôi đã không biết mình đang hướng tới mục đích...   \n",
            "4  Tôi đã không biết mình đang hướng tới mục đích...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                         Anh không nói một lời nào.  contradiction  \n",
            "1                 Anh nói với mẹ rằng anh đã về nhà.     entailment  \n",
            "2  Tôi chưa bao giờ đến Washington nên khi tôi đư...        neutral  \n",
            "3  Tôi biết chính xác những gì tôi cần làm khi tô...  contradiction  \n",
            "4  Tôi đã không hoàn toàn chắc chắn những gì tôi ...     entailment  \n",
            "***************************\n",
            "\n",
            "zh test set:\n",
            "                           premise                          hypothesis  \\\n",
            "0                      他说，妈妈，我回来了。                             他没说一句话。   \n",
            "1                      他说，妈妈，我回来了。                     他告诉他的妈妈他已经回到家了。   \n",
            "2  我不知道我要去干什么还是什么的，所以就去华盛顿指定的地方报到。  我从来没有去过华盛顿，所以当我被派到那里时，为了找地方我都找迷路了。   \n",
            "3  我不知道我要去干什么还是什么的，所以就去华盛顿指定的地方报到。                  在我游行到华盛顿的时候我知道我要什么   \n",
            "4  我不知道我要去干什么还是什么的，所以就去华盛顿指定的地方报到。          我不确定我要做什么，所以我去了华盛顿。我被派去述职。   \n",
            "\n",
            "           label  \n",
            "0  contradiction  \n",
            "1     entailment  \n",
            "2        neutral  \n",
            "3  contradiction  \n",
            "4     entailment  \n",
            "***************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for lang, test_df in lang2test_df.items():\n",
        "    print(f\"{lang} test set:\")\n",
        "    print(test_df.head())\n",
        "    print(\"***************************\\n\")"
      ],
      "id": "362ebef0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bc90924"
      },
      "source": [
        "## mBERT using HuggingFace's transformers library\n",
        "\n",
        "mBERT is a multilingual variant of BERT, which is trained on wikipedia articles in around [100 languages](BertTokenizer). Like monolingual BERT the transformers library also provides pre-trained models and tokenizers for multilingual BERT. To create an instance of one, we only need to specify `\"bert-base-multilingual-cased\"` or `\"bert-base-multilingual-uncased\"` in `BertTokenizer.from_pretrained` and `BertModel.from_pretrained` methods and that's it! See examples below for a demonstration:"
      ],
      "id": "4bc90924"
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "821b8b04"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ],
      "id": "821b8b04"
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "bfddbd7c"
      },
      "outputs": [],
      "source": [
        "mbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
      ],
      "id": "bfddbd7c"
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "042ad78a",
        "outputId": "28db959a-7245-4bec-e94b-07d20ba6b2bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thinking', 'machines']"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "mbert_tokenizer.tokenize(\"thinking machines\")"
      ],
      "id": "042ad78a"
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "deeed8c1",
        "outputId": "c7dc1037-f340-4f21-a55c-8dc51f7a50bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['maquinas', 'de', 'pensar']"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "mbert_tokenizer.tokenize(\"maquinas de pensar\")"
      ],
      "id": "deeed8c1"
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3b7157dc",
        "outputId": "4cead8bb-f543-4465-f0dd-38076285824d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['स', '##ो', '##च', 'म', '##शी', '##न']"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "mbert_tokenizer.tokenize(\"सोच मशीन\")"
      ],
      "id": "3b7157dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b20fe02a"
      },
      "source": [
        "As you can see mBERT's tokenizer works on different languages. We can similarly load a pretrained mbert model and feed data in different languages"
      ],
      "id": "b20fe02a"
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d403c05e",
        "outputId": "f0ff4b88-47d1-4345-9055-254956abd9b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "mbert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
      ],
      "id": "d403c05e"
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b63671ba",
        "outputId": "93b48025-8082-443b-9f81-b287e39ac576"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "mbert_model"
      ],
      "id": "b63671ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96bce87e"
      },
      "source": [
        "As you can see the architecture is identical to the original BERT model. The only thing that is different is the shape of word_embeddings which is 105879 X 768, meaning there are 105879 unique tokens supported by mBERT (uncased). In contrast BERT (uncased) supports 30522 tokens."
      ],
      "id": "96bce87e"
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "edeb75d8",
        "outputId": "f691a911-45d0-405b-b3a3-de9f0196c46a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0295,  0.0307,  0.0245,  ...,  0.0328, -0.0562,  0.0789],\n",
              "                                                        [ 0.2827,  0.4737, -0.1378,  ..., -0.1892,  0.1408, -0.3370],\n",
              "                                                        [ 0.1871,  0.6193,  0.1692,  ...,  0.0987, -0.0519, -0.0380],\n",
              "                                                        [-0.0354,  0.4805, -0.2533,  ...,  0.7390,  0.1286, -0.6764]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 7.3179e-02,  1.0201e-01,  1.0094e-01,  9.1636e-02,  1.4111e-01,\n",
              "                                                         3.3841e-01,  9.0549e-02, -6.1100e-02, -1.3902e-01,  2.4607e-01,\n",
              "                                                        -1.7623e-01, -1.2388e-01,  1.7149e-01, -1.2340e-01, -1.6021e-01,\n",
              "                                                         2.9057e-02,  1.3472e-01,  7.6387e-02,  1.5949e-01,  4.8039e-02,\n",
              "                                                         3.7261e-02, -3.0617e-02,  5.3897e-02,  6.3598e-02,  2.1218e-01,\n",
              "                                                        -1.0945e-01,  2.1902e-01,  8.5052e-02,  2.2458e-01,  2.5002e-01,\n",
              "                                                         1.8784e-01,  1.9675e-01,  1.9028e-01, -6.7974e-02,  1.2689e-01,\n",
              "                                                        -5.0750e-02,  8.9532e-02,  7.3671e-02,  2.0277e-01,  4.1014e-02,\n",
              "                                                         1.2571e-01,  2.5094e-01,  9.7308e-02, -5.8274e-02, -3.0980e-01,\n",
              "                                                         9.3864e-02, -1.3890e-01, -6.5643e-02,  9.9998e-01,  1.3597e-01,\n",
              "                                                         7.1079e-02, -1.7904e-02,  4.0144e-02, -2.4498e-01,  2.1730e-01,\n",
              "                                                         9.9998e-01, -2.9374e-01, -2.0635e-01,  3.9281e-03, -1.2170e-01,\n",
              "                                                        -9.5930e-02,  5.2730e-02,  2.7847e-01,  6.3723e-02, -4.3806e-02,\n",
              "                                                         1.0382e-01, -4.0369e-02,  3.1189e-01,  5.5141e-02,  1.0137e-01,\n",
              "                                                         6.5946e-02, -5.4928e-02,  1.9506e-01,  2.7538e-01, -1.2576e-01,\n",
              "                                                        -3.4104e-02, -1.4140e-01, -9.4945e-02, -8.6547e-02,  1.5645e-01,\n",
              "                                                         1.0313e-01,  4.9405e-02, -2.0957e-01,  1.9060e-01,  2.2057e-02,\n",
              "                                                        -2.0124e-01, -1.5448e-01,  1.0539e-03,  9.0470e-02, -1.2185e-01,\n",
              "                                                         3.1289e-02, -6.4975e-02, -7.1926e-02, -9.2560e-02,  1.5431e-01,\n",
              "                                                        -1.1182e-01, -2.0971e-01,  2.2940e-02,  6.8357e-02, -2.0433e-01,\n",
              "                                                        -2.1997e-02,  1.1978e-01, -4.2444e-03,  1.4974e-01,  1.5300e-01,\n",
              "                                                         6.4773e-02, -1.8277e-01, -1.0238e-01,  6.4092e-02, -2.1379e-02,\n",
              "                                                        -2.0232e-01, -4.0598e-02,  3.0287e-01,  1.1264e-01,  4.2799e-02,\n",
              "                                                        -4.9957e-02, -1.6272e-01, -5.2993e-01,  2.4338e-02,  8.4274e-02,\n",
              "                                                        -7.2513e-02,  9.9998e-01, -8.1502e-02, -1.0315e-01,  1.0807e-01,\n",
              "                                                        -1.6528e-01, -7.4172e-02,  1.7210e-01, -1.7617e-01,  2.3838e-01,\n",
              "                                                        -2.4627e-01, -3.5075e-02, -2.0167e-01,  1.1082e-02, -2.1585e-01,\n",
              "                                                         1.9591e-01,  2.8572e-02, -1.3423e-01, -2.7209e-02, -1.9213e-01,\n",
              "                                                         4.5604e-02,  9.3544e-02,  9.3215e-02,  5.9234e-03,  1.4488e-01,\n",
              "                                                        -1.2675e-01, -8.2117e-02, -2.3946e-02,  5.8369e-02,  2.0040e-01,\n",
              "                                                        -2.7510e-01, -1.5230e-01, -6.6988e-02, -1.4324e-01,  4.8355e-02,\n",
              "                                                        -2.4482e-01,  6.4942e-02, -2.7865e-01,  1.9275e-01, -1.6717e-02,\n",
              "                                                        -2.0453e-01,  2.0115e-01,  2.0163e-01,  5.7707e-02,  1.0814e-01,\n",
              "                                                        -1.4691e-01,  5.0600e-01, -1.4347e-01,  1.4694e-01, -1.6047e-01,\n",
              "                                                         7.4325e-02,  5.1643e-02,  2.3387e-01,  1.5526e-01, -1.1582e-01,\n",
              "                                                        -2.3384e-01,  1.6088e-01, -1.5283e-02,  6.3142e-02, -2.9525e-01,\n",
              "                                                         1.7535e-02, -2.9049e-01, -8.7841e-02,  8.7400e-02,  7.3242e-02,\n",
              "                                                         4.4862e-03, -1.3267e-01, -4.5216e-01, -1.2429e-01, -1.5805e-01,\n",
              "                                                         1.1848e-01,  1.0525e-01,  8.2202e-02,  1.3625e-01,  1.3559e-01,\n",
              "                                                         9.6080e-02, -1.5444e-01, -1.3422e-01, -5.1552e-02,  3.8195e-02,\n",
              "                                                        -3.3402e-01, -2.0950e-02,  5.8303e-02, -1.1204e-01, -2.4186e-01,\n",
              "                                                         2.0704e-01, -1.0256e-01,  9.9998e-01,  1.1866e-01, -7.6896e-02,\n",
              "                                                         1.5445e-01, -4.3845e-02,  2.0678e-02,  1.4142e-02, -1.3867e-01,\n",
              "                                                         1.3031e-01,  1.3122e-01, -3.6649e-03,  1.0144e-01, -1.4299e-01,\n",
              "                                                        -1.6708e-01,  4.9734e-01,  2.1075e-01, -1.8210e-01, -1.5134e-02,\n",
              "                                                        -2.5535e-01, -5.7146e-02, -2.7071e-01,  8.0420e-02, -1.8984e-02,\n",
              "                                                         4.7517e-02,  1.2303e-01,  3.8837e-01, -2.0640e-01,  9.6626e-02,\n",
              "                                                        -2.8383e-01, -1.5654e-01, -1.9583e-01, -2.4416e-01, -1.3667e-01,\n",
              "                                                         6.2990e-02,  1.6517e-01, -9.4204e-02, -7.0088e-02,  2.2586e-01,\n",
              "                                                        -1.5455e-01,  6.8988e-02, -1.5306e-01, -1.4796e-01, -9.1485e-02,\n",
              "                                                         1.8129e-02, -8.6492e-03, -4.7684e-02,  2.5947e-01,  1.6200e-01,\n",
              "                                                         4.8667e-02, -7.7197e-02, -1.5778e-01,  2.4799e-02,  1.3296e-01,\n",
              "                                                         6.9303e-02,  9.7159e-03, -1.6599e-01, -1.1040e-01,  4.5165e-02,\n",
              "                                                        -1.8001e-01, -2.2774e-03,  1.8889e-02,  4.2020e-03,  7.3417e-02,\n",
              "                                                         1.2938e-01,  1.2569e-01, -5.7200e-02,  2.9756e-01, -3.4206e-01,\n",
              "                                                         9.0946e-02,  9.2744e-02, -2.3592e-01,  2.0424e-01,  1.3064e-01,\n",
              "                                                         5.1349e-02, -7.4663e-02,  5.8806e-03, -1.3135e-01,  2.5051e-02,\n",
              "                                                         1.7354e-01,  8.8999e-02, -1.2420e-01,  1.5896e-01,  2.4489e-01,\n",
              "                                                         6.5984e-02, -1.1487e-01, -7.8051e-02,  5.0933e-02,  1.4547e-01,\n",
              "                                                        -1.7255e-01,  2.7150e-01,  9.4294e-02, -1.1215e-01, -1.3279e-01,\n",
              "                                                         9.2363e-02, -1.3826e-01, -9.9182e-02,  1.7942e-02,  1.6031e-01,\n",
              "                                                         2.1074e-01, -1.6457e-01, -5.5597e-02, -1.2069e-01,  1.0694e-02,\n",
              "                                                         7.2653e-02, -6.5961e-02, -1.9477e-01,  2.1003e-01, -3.0987e-01,\n",
              "                                                         9.4052e-02,  1.9550e-01, -2.8578e-01, -7.0935e-03,  1.2259e-02,\n",
              "                                                         1.1221e-01,  9.9998e-01, -1.5399e-01, -1.5067e-01, -9.9998e-01,\n",
              "                                                        -4.0517e-01, -2.2675e-01,  2.4685e-01,  1.2637e-01, -2.0596e-01,\n",
              "                                                         9.4711e-03,  9.8534e-02, -1.5658e-01, -1.5616e-01,  1.1421e-01,\n",
              "                                                         3.4512e-01, -1.1152e-01,  9.7742e-02,  7.4420e-02,  1.6055e-02,\n",
              "                                                         9.3673e-02, -9.9997e-01,  1.3721e-02,  1.0505e-01,  4.5990e-02,\n",
              "                                                        -2.7098e-02, -1.7936e-01, -1.9624e-01, -7.5425e-02,  1.3567e-01,\n",
              "                                                         1.2713e-01,  1.2091e-01,  2.1557e-02, -1.5238e-01, -3.4905e-01,\n",
              "                                                         1.1866e-01, -5.9070e-02,  3.0714e-02, -1.7615e-01, -1.8791e-01,\n",
              "                                                         4.0728e-02,  1.5794e-01,  4.1041e-03,  2.5339e-01, -9.1826e-02,\n",
              "                                                         9.0278e-02, -1.4581e-01,  9.9998e-01,  1.3606e-01, -9.2719e-02,\n",
              "                                                         9.9998e-01,  2.4808e-01,  2.8235e-02, -3.3395e-01,  9.1213e-02,\n",
              "                                                         6.6392e-02,  9.9998e-01,  6.3202e-02, -1.9145e-01,  1.9956e-02,\n",
              "                                                        -8.7200e-02,  1.6400e-01,  3.1808e-01, -5.4950e-02, -9.9998e-01,\n",
              "                                                         3.9525e-02,  1.3308e-01,  8.9568e-02, -2.0165e-02,  9.9998e-01,\n",
              "                                                        -4.1041e-02, -9.1356e-02,  7.8816e-03,  2.8616e-01,  7.6935e-02,\n",
              "                                                         3.1532e-01,  1.8350e-01,  2.7013e-02, -1.6635e-01, -6.1900e-02,\n",
              "                                                        -7.2002e-02, -2.7173e-01, -1.1778e-01,  5.0166e-02, -6.7673e-02,\n",
              "                                                        -2.2937e-02, -1.5914e-01, -6.9602e-02,  2.1845e-02,  9.9998e-01,\n",
              "                                                        -5.6357e-02,  1.2465e-01,  5.5937e-02,  3.0309e-02,  8.3948e-02,\n",
              "                                                        -3.2562e-02,  9.1417e-02, -2.0854e-01,  2.4348e-01, -2.0648e-01,\n",
              "                                                         1.2498e-01, -2.9970e-02, -2.4258e-03, -1.8928e-01, -5.9992e-02,\n",
              "                                                        -1.8654e-01,  1.1466e-01,  3.2974e-02,  2.7450e-01,  9.8092e-02,\n",
              "                                                        -5.1885e-03, -4.1000e-02,  1.3183e-01,  3.1027e-04,  2.5039e-01,\n",
              "                                                        -1.2271e-01,  9.9998e-01,  1.4857e-01, -1.0520e-01,  4.4376e-02,\n",
              "                                                        -8.9653e-02, -1.6761e-01, -1.0846e-01, -2.6062e-01,  3.6854e-01,\n",
              "                                                         7.4351e-02,  1.4631e-01,  6.7138e-02,  2.0981e-01,  4.1745e-03,\n",
              "                                                        -8.7262e-02,  1.0625e-01,  4.5029e-02, -4.9726e-02, -1.6090e-01,\n",
              "                                                        -9.9999e-01, -2.4006e-01,  1.2973e-01, -1.7996e-01, -1.4771e-01,\n",
              "                                                         9.7282e-02, -2.4893e-01, -2.8951e-02, -1.0087e-01, -2.5671e-02,\n",
              "                                                         5.3565e-03, -1.7670e-01, -1.5131e-01,  7.0804e-02, -1.4243e-01,\n",
              "                                                        -1.8981e-01,  1.7267e-01,  1.5724e-01,  2.5108e-01,  3.8738e-02,\n",
              "                                                         2.4062e-01,  1.2826e-01, -1.4060e-02,  1.3581e-01, -2.5359e-02,\n",
              "                                                         2.8072e-03, -2.6812e-01, -2.8872e-01, -1.3352e-01, -1.5422e-01,\n",
              "                                                         1.3957e-01, -1.9933e-01,  3.8098e-02,  5.4995e-02, -1.1654e-01,\n",
              "                                                         2.3261e-01,  1.3446e-01,  4.0561e-02, -1.5145e-01,  2.4640e-02,\n",
              "                                                        -4.3276e-03,  6.3809e-02, -2.7966e-01,  1.8674e-01, -9.3060e-02,\n",
              "                                                         1.3904e-01, -3.1102e-02, -1.2904e-01,  1.3963e-01, -1.9823e-01,\n",
              "                                                         1.3823e-01,  1.3177e-01,  1.4865e-01, -4.3583e-02, -1.7827e-01,\n",
              "                                                        -2.8847e-02, -7.4497e-03,  2.0755e-01,  7.0420e-02, -8.1432e-02,\n",
              "                                                        -1.1339e-01, -7.5351e-02, -2.0457e-01,  3.9577e-02, -2.0639e-02,\n",
              "                                                        -9.9998e-01,  2.6042e-03, -1.4317e-01, -1.2968e-01,  1.6345e-01,\n",
              "                                                         1.3522e-02,  2.9230e-01, -7.4525e-02, -2.8081e-01,  2.1194e-01,\n",
              "                                                         4.2108e-03, -2.3653e-01, -5.0939e-02,  7.8465e-02,  8.5713e-03,\n",
              "                                                         4.9472e-02, -9.9998e-01, -2.4342e-02,  6.6553e-02, -1.9232e-01,\n",
              "                                                         1.0921e-01,  1.4569e-01, -3.5886e-02,  7.3304e-02, -6.5317e-02,\n",
              "                                                         6.8486e-01,  6.1876e-02,  8.5777e-02, -8.7385e-02, -8.7729e-02,\n",
              "                                                        -1.3302e-01, -1.5858e-01, -1.5562e-01,  1.6459e-01,  3.1980e-03,\n",
              "                                                         1.2599e-01, -1.6958e-01,  2.5404e-01,  4.9494e-02,  3.7814e-01,\n",
              "                                                         1.5938e-01, -7.9063e-02,  5.5768e-02, -5.4164e-02,  1.7684e-01,\n",
              "                                                         2.8788e-02,  2.4044e-01, -1.7441e-01,  7.5438e-02,  3.1538e-02,\n",
              "                                                        -1.3379e-01,  1.4529e-01,  1.6321e-01,  1.8002e-02,  2.7364e-01,\n",
              "                                                         1.0350e-01,  1.6075e-01, -6.0345e-02, -6.6107e-02,  1.5257e-02,\n",
              "                                                         5.3042e-02, -2.2797e-01,  2.5812e-02,  5.0953e-02,  6.9829e-02,\n",
              "                                                        -1.4107e-01,  1.0170e-01, -1.7497e-01,  6.7592e-02, -2.2314e-01,\n",
              "                                                        -1.6602e-01, -1.5866e-01,  5.3789e-02, -5.5261e-02, -1.7660e-01,\n",
              "                                                         1.8860e-01, -1.3061e-01, -2.0776e-01, -1.0591e-01,  4.6544e-02,\n",
              "                                                         9.5898e-02, -7.5415e-02, -2.9110e-02,  1.4028e-01,  2.0851e-01,\n",
              "                                                        -2.7833e-02,  1.4745e-01,  9.1486e-02, -5.3949e-02,  6.6658e-02,\n",
              "                                                         2.0588e-01, -1.2952e-01, -2.6022e-01,  1.8500e-01,  8.4619e-02,\n",
              "                                                         1.4916e-02,  1.7777e-01, -3.8061e-02, -2.3495e-01,  1.3902e-01,\n",
              "                                                         3.2566e-02, -4.0397e-02, -3.6004e-02,  1.2015e-01, -1.0909e-01,\n",
              "                                                        -8.8604e-02,  1.1901e-01, -9.9998e-01,  1.0674e-01, -8.9303e-02,\n",
              "                                                        -4.3718e-02,  2.6640e-01,  2.2693e-01,  1.5056e-01,  1.4210e-01,\n",
              "                                                         1.6559e-02, -2.3216e-01, -2.5339e-01, -1.7109e-02, -9.0951e-02,\n",
              "                                                        -3.2910e-01,  7.1875e-02, -1.3863e-01, -7.9049e-02,  1.5858e-01,\n",
              "                                                        -2.7387e-01, -1.9463e-01, -9.5768e-02, -6.6397e-02,  1.5667e-01,\n",
              "                                                         8.5604e-02,  3.7488e-03, -1.7165e-01, -6.4463e-02,  4.7902e-02,\n",
              "                                                        -2.8880e-01, -2.5597e-01,  2.0448e-01,  2.2661e-01,  1.8551e-01,\n",
              "                                                        -4.0198e-01,  1.8714e-01, -1.2346e-01, -6.3953e-02,  5.8400e-03,\n",
              "                                                         1.6813e-01,  7.9837e-04, -1.1464e-01,  2.6567e-01, -1.1165e-01,\n",
              "                                                        -2.1061e-01,  6.7415e-02, -2.5445e-01,  1.1992e-01, -6.3134e-01,\n",
              "                                                         1.9971e-01,  9.3713e-02,  3.0368e-01, -1.5028e-01,  2.2328e-01,\n",
              "                                                         8.5727e-02,  4.6135e-02,  5.7842e-03,  1.4339e-01, -7.3669e-03,\n",
              "                                                         2.8754e-01, -3.1202e-02,  2.2612e-01,  3.4620e-01, -9.9998e-01,\n",
              "                                                        -3.6616e-02, -5.4562e-02, -1.5097e-01, -9.9998e-01, -8.8907e-02,\n",
              "                                                         1.2939e-01,  5.8688e-02,  1.5712e-01,  5.9310e-02,  1.4333e-01,\n",
              "                                                         1.9591e-01, -1.4970e-01, -1.1364e-01,  1.6807e-01, -1.9834e-01,\n",
              "                                                        -5.9953e-02,  1.9273e-01, -1.3947e-01, -1.7666e-01,  1.1213e-01,\n",
              "                                                         1.8712e-01,  1.6260e-01,  7.3317e-02, -2.6980e-01,  4.3758e-01,\n",
              "                                                        -8.4492e-02, -4.7005e-02, -3.3171e-01, -1.0506e-01,  2.1018e-01,\n",
              "                                                        -6.6405e-02,  5.1140e-02,  1.3922e-01,  2.2156e-01, -8.6738e-02,\n",
              "                                                        -1.8886e-01, -5.7752e-02, -8.9208e-02,  2.6356e-01,  9.5995e-02,\n",
              "                                                         1.1035e-01, -2.0700e-02,  2.8573e-01, -1.5227e-01, -6.7774e-02,\n",
              "                                                        -1.2151e-03, -3.9225e-03,  2.1186e-01, -1.6579e-01, -1.0625e-01,\n",
              "                                                         3.1726e-01, -1.9441e-01, -8.0780e-02,  1.5680e-01,  1.2033e-01,\n",
              "                                                        -1.8308e-01, -6.0178e-02, -9.5075e-02,  2.9383e-01, -9.9998e-01,\n",
              "                                                        -1.3482e-02, -5.7824e-02,  1.3638e-01,  1.1943e-02, -9.4662e-02,\n",
              "                                                         6.3522e-01,  2.0999e-01, -8.0717e-03,  2.4701e-01, -2.1165e-01,\n",
              "                                                        -8.7867e-02, -1.8082e-01,  1.1048e-01,  4.5319e-02, -9.6413e-03,\n",
              "                                                         1.1225e-02, -1.1681e-01, -1.1469e-01]], grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "en_sent = \"thinking machines\"\n",
        "tokenizer_output = mbert_tokenizer(en_sent, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "mbert_model(input_ids, attention_mask = attn_mask)"
      ],
      "id": "edeb75d8"
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "829a3a77",
        "outputId": "0bb40f77-caa6-439a-b766-98b2a5323cb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0657, -0.0614,  0.0060,  ..., -0.0240, -0.0420, -0.0673],\n",
              "                                                        [ 0.2430,  0.5252, -0.0694,  ..., -0.0108,  0.5518, -0.4204],\n",
              "                                                        [-0.2587, -0.2086,  0.1115,  ..., -0.7734,  0.1412, -0.9133],\n",
              "                                                        [-0.1032,  0.7808,  0.1022,  ...,  0.0870,  0.1456, -0.5586],\n",
              "                                                        [-0.0765,  0.3234, -0.2513,  ...,  0.3950,  0.2862, -1.1907]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 1.4176e-01,  3.4401e-02,  1.5948e-01,  1.7631e-01,  1.8552e-01,\n",
              "                                                         4.2113e-01,  1.4076e-01, -1.4554e-01, -1.7636e-01,  2.9590e-01,\n",
              "                                                        -1.8210e-01, -2.1683e-01,  2.2756e-01, -1.7093e-01, -1.8114e-01,\n",
              "                                                         5.4472e-02,  1.9600e-01,  1.2176e-01,  2.0155e-01,  1.3893e-01,\n",
              "                                                        -4.2202e-02, -8.1407e-02,  8.8412e-02, -1.3793e-02,  2.7080e-01,\n",
              "                                                        -1.6806e-01,  2.6696e-01,  1.2918e-01,  3.6292e-01,  2.9422e-01,\n",
              "                                                         2.0838e-01,  2.5432e-01,  2.5948e-01, -1.1543e-01,  2.1006e-01,\n",
              "                                                         2.0638e-02,  3.6321e-02,  1.2488e-01,  2.3027e-01,  8.2570e-02,\n",
              "                                                         1.4708e-01,  4.4298e-01,  1.5398e-01, -1.5360e-01, -3.4018e-01,\n",
              "                                                         2.2819e-01, -1.9345e-01, -8.5304e-02,  9.9999e-01,  1.9508e-01,\n",
              "                                                         8.9446e-02, -1.4110e-01,  1.0632e-01, -2.7657e-01,  2.4497e-01,\n",
              "                                                         9.9999e-01, -3.4471e-01, -2.3381e-01,  5.2617e-02, -1.3738e-01,\n",
              "                                                        -2.0170e-01,  7.5959e-02,  3.1516e-01,  1.3019e-01, -1.1624e-01,\n",
              "                                                         1.2369e-01, -6.2276e-02,  3.3121e-01,  1.2949e-02,  1.1194e-01,\n",
              "                                                         1.0270e-01, -1.0232e-01,  2.2664e-01,  3.0402e-01, -1.8596e-01,\n",
              "                                                         4.0614e-02, -2.0928e-01, -2.0389e-01, -1.9426e-01,  1.9920e-01,\n",
              "                                                         1.9141e-01,  8.9631e-02, -2.5999e-01,  2.3049e-01, -9.8978e-02,\n",
              "                                                        -2.9516e-01, -2.1329e-01, -7.9474e-02,  1.3188e-01, -2.2406e-01,\n",
              "                                                         9.4448e-02, -1.0134e-01, -9.9620e-02, -1.3253e-01,  1.6258e-01,\n",
              "                                                        -1.6331e-01, -2.5396e-01, -7.3900e-02,  1.3007e-01, -2.8521e-01,\n",
              "                                                        -5.7481e-02,  1.6074e-01,  4.0952e-02,  1.1781e-01,  2.0784e-01,\n",
              "                                                         2.3460e-01, -2.5247e-01, -1.3074e-01, -1.0675e-02, -9.2086e-03,\n",
              "                                                        -2.2128e-01,  4.4169e-02,  4.5098e-01,  2.1983e-01,  8.7659e-02,\n",
              "                                                         2.7385e-02, -1.8454e-01, -7.4263e-01, -5.1756e-02,  1.4141e-01,\n",
              "                                                        -3.4279e-02,  9.9999e-01, -1.6274e-01, -1.8138e-01,  1.5650e-01,\n",
              "                                                        -2.6184e-01, -8.8531e-02,  2.2055e-01, -2.1966e-01,  3.1545e-01,\n",
              "                                                        -2.9460e-01, -8.4367e-02, -2.5458e-01,  4.0762e-02, -2.7454e-01,\n",
              "                                                         2.8245e-01,  8.0520e-02, -1.1288e-01,  5.4163e-02, -2.4504e-01,\n",
              "                                                         7.4324e-02,  1.4228e-01,  1.4912e-01,  1.3070e-01,  1.9802e-01,\n",
              "                                                        -1.9757e-01, -9.4446e-02,  1.2255e-02, -4.4503e-02,  2.2847e-01,\n",
              "                                                        -3.0940e-01, -2.4367e-01, -3.6847e-02, -1.9609e-01,  4.9530e-03,\n",
              "                                                        -3.1056e-01,  1.1333e-01, -3.2450e-01,  2.2749e-01,  7.3080e-02,\n",
              "                                                        -2.3836e-01,  2.3000e-01,  2.5693e-01,  1.3141e-01,  1.7909e-01,\n",
              "                                                        -1.7676e-01,  7.1778e-01, -1.9316e-01,  2.4662e-01, -2.3022e-01,\n",
              "                                                        -7.1705e-02,  1.1786e-01,  2.7383e-01,  1.9487e-01, -1.4186e-01,\n",
              "                                                        -2.8495e-01,  2.0092e-01, -7.7066e-02,  1.3877e-01, -4.7296e-01,\n",
              "                                                         7.2583e-02, -3.7574e-01, -1.2160e-01,  1.4101e-01,  3.9109e-02,\n",
              "                                                         4.1496e-02, -2.3796e-01, -6.5368e-01, -4.6148e-02, -2.4818e-01,\n",
              "                                                         1.7878e-01,  1.8163e-01,  1.3553e-01,  2.3442e-01,  1.8634e-01,\n",
              "                                                        -1.0071e-02, -1.9418e-01, -2.2230e-01, -7.7779e-02,  1.1157e-01,\n",
              "                                                        -4.3844e-01,  4.1400e-02, -5.1808e-02, -1.6167e-01, -2.8458e-01,\n",
              "                                                         2.7338e-01, -1.7314e-01,  9.9999e-01,  6.0498e-02, -1.6026e-01,\n",
              "                                                         1.6814e-01, -1.4584e-01, -5.7390e-02, -5.6743e-02, -2.1679e-01,\n",
              "                                                         1.5827e-01,  1.6530e-01, -8.8480e-02,  1.8519e-01, -2.3680e-01,\n",
              "                                                        -2.3501e-01,  6.7255e-01,  2.8943e-01, -2.2693e-01, -6.8995e-02,\n",
              "                                                        -2.8145e-01, -1.9822e-02, -3.4394e-01,  2.2847e-01, -1.2438e-01,\n",
              "                                                         8.7177e-02,  1.5882e-01,  4.0037e-01, -2.5549e-01,  1.4033e-01,\n",
              "                                                        -3.3692e-01, -2.1416e-01, -2.2396e-01, -4.0181e-01, -2.0030e-01,\n",
              "                                                         1.0835e-01,  1.6841e-01, -1.4286e-01, -5.5853e-02,  2.6443e-01,\n",
              "                                                        -2.0406e-01,  1.3307e-01, -2.1589e-01, -2.5919e-01, -1.5416e-01,\n",
              "                                                         4.1412e-02, -1.3100e-01, -1.1098e-01,  2.7552e-01,  1.8748e-01,\n",
              "                                                         1.2460e-01, -1.0504e-01, -1.6342e-01,  8.9278e-02,  1.9794e-01,\n",
              "                                                         1.5078e-01,  3.2018e-02, -2.2913e-01, -1.0459e-01, -1.0096e-02,\n",
              "                                                        -2.5375e-01,  6.5590e-02,  6.1361e-02, -1.2471e-01,  1.0422e-01,\n",
              "                                                         3.1563e-01,  2.0883e-01, -1.0141e-01,  3.4946e-01, -4.2757e-01,\n",
              "                                                         1.1210e-01,  1.7992e-01, -2.4741e-01,  2.5516e-01,  1.5931e-01,\n",
              "                                                        -1.8456e-02, -1.2316e-01,  7.8277e-02, -1.6251e-01,  7.2417e-02,\n",
              "                                                         2.2503e-01,  1.5575e-01, -1.6483e-01,  1.4196e-01,  3.0710e-01,\n",
              "                                                         1.3345e-01, -1.7176e-01, -1.1518e-01,  1.4071e-01,  2.1860e-01,\n",
              "                                                        -2.0526e-01,  3.1394e-01,  1.8230e-01, -1.8522e-01, -1.5367e-01,\n",
              "                                                         1.1750e-01, -1.9448e-01, -1.7304e-01, -5.1064e-02,  2.6404e-01,\n",
              "                                                         2.3060e-01, -2.1061e-01, -1.2781e-01, -2.0407e-01, -4.5289e-02,\n",
              "                                                         8.3658e-02, -1.5178e-01, -2.6070e-01,  2.2416e-01, -3.5798e-01,\n",
              "                                                         1.1783e-01,  2.2576e-01, -3.2027e-01, -5.1004e-02, -5.5693e-02,\n",
              "                                                         1.8284e-01,  9.9999e-01, -2.1127e-01, -1.3878e-01, -9.9999e-01,\n",
              "                                                        -4.6576e-01, -2.9456e-01,  2.7522e-01,  1.6385e-01, -2.7458e-01,\n",
              "                                                        -7.8472e-02,  1.3114e-01, -1.9778e-01, -2.3622e-01,  2.1868e-01,\n",
              "                                                         3.8457e-01, -1.8298e-01,  1.2252e-01,  1.0747e-01,  2.9732e-02,\n",
              "                                                         1.2897e-01, -9.9998e-01, -4.7202e-02,  1.3701e-02, -3.2895e-02,\n",
              "                                                        -7.5726e-02, -2.2144e-01, -2.1903e-01, -1.2936e-01,  1.8736e-01,\n",
              "                                                         1.9446e-01,  1.7180e-01, -7.0668e-02, -2.5061e-01, -3.4393e-01,\n",
              "                                                         1.5563e-01, -1.3025e-01,  6.0536e-02, -2.0935e-01, -2.5558e-01,\n",
              "                                                         1.0602e-01,  2.3848e-01,  1.9070e-02,  2.9148e-01, -1.4300e-01,\n",
              "                                                         1.8768e-01, -2.1562e-01,  9.9999e-01,  2.1174e-01, -1.7241e-01,\n",
              "                                                         9.9999e-01,  2.9368e-01, -2.0802e-02, -4.4702e-01,  1.0769e-01,\n",
              "                                                         1.0370e-01,  9.9999e-01,  1.5182e-01, -2.5394e-01, -8.5451e-02,\n",
              "                                                         4.6754e-02,  2.4464e-01,  3.8935e-01, -5.4867e-03, -9.9999e-01,\n",
              "                                                         1.0994e-01,  1.7869e-01,  3.3436e-03, -4.7389e-02,  9.9999e-01,\n",
              "                                                        -9.2318e-02, -1.8041e-01,  7.8401e-02,  3.1596e-01,  1.4380e-01,\n",
              "                                                         3.7415e-01,  1.8647e-01, -2.8230e-02, -2.3531e-01, -9.2129e-02,\n",
              "                                                        -1.3040e-01, -3.2820e-01, -1.7585e-01,  1.0518e-01, -1.1582e-01,\n",
              "                                                        -1.0651e-01, -2.4714e-01,  1.1307e-02, -5.5399e-02,  9.9999e-01,\n",
              "                                                        -2.4822e-02,  1.9087e-01,  6.9512e-02,  1.3101e-01,  1.0372e-01,\n",
              "                                                        -7.8402e-02, -1.4170e-02, -2.8408e-01,  2.8636e-01, -2.6532e-01,\n",
              "                                                         1.8592e-01,  7.6087e-02,  5.1574e-02, -2.3418e-01, -1.0005e-01,\n",
              "                                                        -2.0759e-01,  1.8586e-01,  6.1952e-02,  4.4869e-01,  1.5651e-01,\n",
              "                                                        -9.8914e-02, -1.2176e-01,  1.8979e-01, -2.7777e-02,  2.8522e-01,\n",
              "                                                        -1.6327e-01,  9.9999e-01,  2.0271e-01, -1.7230e-01, -9.5021e-03,\n",
              "                                                        -1.1205e-01, -2.2409e-01, -1.8732e-01, -2.7898e-01,  5.4942e-01,\n",
              "                                                         1.3951e-01,  9.7582e-02,  1.6390e-01,  2.6419e-01, -7.2977e-02,\n",
              "                                                        -9.9676e-02,  1.4631e-01,  3.5513e-02,  7.0370e-03, -2.9917e-01,\n",
              "                                                        -9.9999e-01, -2.7512e-01,  1.8115e-01, -3.4174e-01, -2.1146e-01,\n",
              "                                                         3.7809e-02, -3.2011e-01, -8.5779e-02, -3.9099e-02,  8.2516e-03,\n",
              "                                                         8.7735e-02, -2.5740e-01, -2.0243e-01,  1.1256e-01, -1.3412e-01,\n",
              "                                                        -2.1730e-01,  1.9210e-01,  2.7734e-01,  2.7189e-01, -9.4020e-02,\n",
              "                                                         2.7952e-01,  1.8280e-01,  1.1657e-02,  2.1934e-01, -7.1973e-02,\n",
              "                                                         6.1104e-02, -3.5227e-01, -3.3142e-01, -1.9634e-01, -1.9735e-01,\n",
              "                                                         2.0644e-01, -2.4785e-01, -3.3866e-02,  1.2497e-01, -1.9365e-01,\n",
              "                                                         2.6553e-01,  1.8153e-01,  8.9659e-03, -1.6964e-01,  8.8955e-02,\n",
              "                                                        -6.2140e-02,  1.4524e-01, -3.3219e-01,  2.3616e-01, -1.8622e-02,\n",
              "                                                         2.2127e-01, -1.5129e-01, -6.2765e-02,  1.9729e-01, -2.5878e-01,\n",
              "                                                         1.7096e-01,  1.6829e-01,  1.9079e-01, -1.1256e-01, -2.2692e-01,\n",
              "                                                         9.4869e-04,  3.8270e-02,  2.4418e-01,  1.2216e-01, -1.7033e-01,\n",
              "                                                        -1.5731e-01, -1.6348e-01, -2.3694e-01, -3.7734e-04,  5.7342e-02,\n",
              "                                                        -9.9999e-01, -6.2483e-02, -1.4103e-01, -1.9935e-01,  2.1452e-01,\n",
              "                                                         1.0831e-01,  3.4353e-01, -6.4320e-02, -4.8609e-01,  2.9674e-01,\n",
              "                                                        -7.2625e-02, -2.7103e-01, -4.3679e-02,  1.3353e-01, -5.0690e-02,\n",
              "                                                         5.9567e-02, -9.9999e-01,  4.1516e-02,  1.8530e-01, -2.1969e-01,\n",
              "                                                         1.4529e-01,  1.9153e-01, -1.0433e-01,  1.0763e-01, -8.1715e-02,\n",
              "                                                         8.5897e-01,  8.7412e-02,  1.0658e-01, -1.1136e-01, -3.6796e-03,\n",
              "                                                        -1.7956e-01, -2.8567e-01, -1.8426e-01,  1.7989e-01, -8.2678e-03,\n",
              "                                                         1.4666e-01, -2.3866e-01,  2.7470e-01,  8.0881e-02,  4.1714e-01,\n",
              "                                                         1.9624e-01, -7.9879e-02,  6.6923e-02, -1.3577e-01,  2.4110e-01,\n",
              "                                                         5.0170e-02,  2.7440e-01, -1.0605e-01,  1.7907e-02,  1.4192e-01,\n",
              "                                                        -1.9542e-01,  1.7607e-01,  1.6831e-01,  8.5067e-02,  3.2943e-01,\n",
              "                                                         2.0415e-01,  2.2167e-01, -1.0445e-01, -1.1645e-01,  1.0808e-01,\n",
              "                                                         1.1252e-01, -2.6779e-01,  5.5929e-02,  1.6692e-01,  3.4567e-02,\n",
              "                                                        -6.8955e-02,  9.7206e-02, -2.1741e-01,  1.2839e-01, -2.3346e-01,\n",
              "                                                        -2.0766e-01, -2.3677e-01,  1.1460e-01, -1.0576e-01, -2.5598e-01,\n",
              "                                                         2.5922e-01, -1.7811e-01, -2.4270e-01, -1.0195e-01, -4.3533e-02,\n",
              "                                                         2.1582e-01, -1.4364e-01, -7.8587e-02,  1.8210e-01,  2.7148e-01,\n",
              "                                                         6.4058e-02,  2.2729e-01,  1.4698e-01, -1.0750e-01,  5.3824e-02,\n",
              "                                                         1.9585e-01, -2.0472e-01, -3.1686e-01,  2.0575e-01,  1.4279e-01,\n",
              "                                                         7.3550e-02,  1.9425e-01,  1.9031e-02, -2.7806e-01,  1.7672e-01,\n",
              "                                                         8.6702e-02, -5.5432e-02, -1.4306e-01,  1.4652e-01, -1.5505e-01,\n",
              "                                                        -9.4883e-02,  1.9275e-01, -9.9999e-01,  7.2696e-02, -1.1300e-01,\n",
              "                                                        -1.3813e-01,  3.3507e-01,  2.6896e-01,  1.7496e-01,  2.0019e-01,\n",
              "                                                        -8.8428e-03, -2.8412e-01, -2.7504e-01, -6.9042e-02, -5.6403e-02,\n",
              "                                                        -5.0013e-01,  1.2411e-01, -1.8666e-02, -8.8522e-02,  1.5650e-01,\n",
              "                                                        -4.0721e-01, -2.3049e-01, -1.5754e-01, -8.8076e-02,  2.1587e-01,\n",
              "                                                         1.3142e-01,  6.6759e-02, -3.6864e-01, -1.3832e-01, -1.4163e-02,\n",
              "                                                        -3.2093e-01, -2.9088e-01,  2.3202e-01,  2.5552e-01,  2.6093e-01,\n",
              "                                                        -5.5880e-01,  2.3389e-01, -2.1585e-01, -1.0226e-01,  4.9435e-02,\n",
              "                                                         1.6739e-01,  6.9797e-02, -1.2943e-01,  3.1545e-01, -1.4997e-01,\n",
              "                                                        -2.1408e-01,  1.0682e-01, -3.2121e-01,  1.3959e-01, -8.3104e-01,\n",
              "                                                         2.2084e-01,  1.2266e-01,  3.7062e-01, -2.3118e-01,  2.3138e-01,\n",
              "                                                         1.2532e-01,  1.1596e-01,  6.0222e-02,  1.9363e-01,  3.7853e-02,\n",
              "                                                         3.1746e-01, -8.0981e-02,  2.6098e-01,  3.9145e-01, -9.9999e-01,\n",
              "                                                        -7.6867e-02, -9.7705e-02, -1.9425e-01, -9.9999e-01, -1.6877e-01,\n",
              "                                                         1.6510e-01,  1.0897e-01,  1.8924e-01,  1.5639e-02,  2.1435e-01,\n",
              "                                                         2.6257e-01, -2.0205e-01, -1.9933e-01,  2.1862e-01, -2.7635e-01,\n",
              "                                                        -2.1669e-02,  2.4839e-01, -2.1092e-01, -1.9723e-01,  1.4701e-01,\n",
              "                                                         3.0229e-01,  3.7635e-01, -1.4870e-02, -4.7076e-01,  4.4111e-01,\n",
              "                                                        -1.0094e-01, -9.4811e-02, -4.3551e-01, -1.5859e-01,  2.5735e-01,\n",
              "                                                        -1.3036e-02,  1.0823e-01,  1.4470e-01,  3.0342e-01, -9.2594e-02,\n",
              "                                                        -2.0271e-01,  4.7906e-02, -1.3411e-01,  3.1351e-01,  1.6065e-01,\n",
              "                                                         1.6702e-01, -7.7265e-02,  4.4583e-01, -2.3032e-01, -1.3456e-01,\n",
              "                                                         4.6926e-02, -6.6956e-02,  3.8225e-01, -1.8512e-01, -1.9204e-01,\n",
              "                                                         3.7974e-01, -2.1666e-01, -6.7088e-02,  2.1756e-01,  1.3400e-01,\n",
              "                                                        -2.4468e-01, -1.2421e-01, -1.3409e-01,  3.2119e-01, -9.9999e-01,\n",
              "                                                        -5.9038e-02, -1.2530e-01,  2.2247e-01,  5.1446e-02, -1.5484e-01,\n",
              "                                                         8.1418e-01,  3.5868e-01,  3.3257e-02,  3.1401e-01, -2.9680e-01,\n",
              "                                                        -1.2265e-01, -2.9195e-01,  1.5110e-01,  8.8186e-02, -4.4364e-02,\n",
              "                                                         3.3199e-02, -1.5291e-01, -1.7318e-01]], grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "es_sent = \"maquinas de pensar\"\n",
        "tokenizer_output = mbert_tokenizer(es_sent, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "mbert_model(input_ids, attention_mask = attn_mask)"
      ],
      "id": "829a3a77"
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6b0d2bf6",
        "outputId": "ad86846e-c7b7-401f-d2e9-a36fdee39fbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0183,  0.0577,  0.1038,  ..., -0.0349, -0.0897, -0.0034],\n",
              "                                                        [-0.2319,  0.0178, -0.0228,  ..., -0.4928,  0.1712, -0.3489],\n",
              "                                                        [-0.1214,  0.0804,  0.2790,  ..., -0.0647, -0.2073, -0.6718],\n",
              "                                                        ...,\n",
              "                                                        [-0.0835,  0.4992, -0.1798,  ...,  0.5467, -0.4407,  0.0024],\n",
              "                                                        [ 0.1764,  0.7496,  0.2607,  ...,  0.2637, -0.2617, -0.2716],\n",
              "                                                        [-0.3781,  0.5859,  0.2654,  ...,  0.3965, -0.5680, -0.8965]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 1.9663e-01,  3.6831e-02,  1.9092e-01,  1.6675e-01,  2.1876e-01,\n",
              "                                                         4.5212e-01,  1.5738e-01, -1.8443e-01, -1.9007e-01,  2.7668e-01,\n",
              "                                                        -2.6375e-01, -2.2846e-01,  2.6193e-01, -1.7115e-01, -2.4437e-01,\n",
              "                                                         5.3468e-02,  1.8985e-01,  1.4454e-01,  2.6989e-01,  1.0231e-01,\n",
              "                                                        -7.4351e-02, -1.1409e-01,  1.3042e-01, -5.6443e-02,  3.0324e-01,\n",
              "                                                        -1.6460e-01,  2.7847e-01,  1.3497e-01,  4.5047e-01,  2.9605e-01,\n",
              "                                                         2.5428e-01,  2.3158e-01,  2.5374e-01, -7.5846e-02,  2.3163e-01,\n",
              "                                                         5.5427e-02,  1.1144e-02,  1.2426e-01,  2.6061e-01,  5.3806e-02,\n",
              "                                                         1.6303e-01,  5.1142e-01,  1.3269e-01, -1.3358e-01, -3.7102e-01,\n",
              "                                                         2.4315e-01, -1.8334e-01, -1.0740e-01,  9.9999e-01,  2.0407e-01,\n",
              "                                                         1.4211e-01, -2.0426e-01,  1.3370e-01, -3.2816e-01,  2.5923e-01,\n",
              "                                                         9.9999e-01, -3.7073e-01, -2.9902e-01,  4.3326e-02, -1.4992e-01,\n",
              "                                                        -2.3615e-01,  4.2203e-02,  3.4740e-01,  1.3696e-01, -1.5259e-01,\n",
              "                                                         7.5456e-02, -1.1221e-01,  3.4851e-01, -1.9166e-03,  9.3484e-02,\n",
              "                                                         9.4926e-02, -1.3147e-01,  2.1034e-01,  3.0040e-01, -1.8503e-01,\n",
              "                                                         8.6255e-02, -2.2896e-01, -1.7900e-01, -1.6472e-01,  1.9012e-01,\n",
              "                                                         2.1268e-01,  8.2321e-02, -2.9702e-01,  2.2585e-01, -1.8244e-01,\n",
              "                                                        -3.2864e-01, -2.8098e-01, -1.1694e-01,  1.7625e-01, -1.6935e-01,\n",
              "                                                         1.3944e-01, -1.1736e-01, -1.0675e-01, -1.4355e-01,  1.8238e-01,\n",
              "                                                        -1.6324e-01, -2.8235e-01, -8.2830e-02,  1.2819e-01, -2.6289e-01,\n",
              "                                                        -6.9057e-02,  1.9388e-01,  5.7897e-02,  4.5263e-02,  2.4417e-01,\n",
              "                                                         3.3384e-01, -2.9188e-01, -1.8632e-01, -1.7193e-02,  3.4798e-02,\n",
              "                                                        -2.2559e-01,  6.8057e-02,  4.9111e-01,  2.5987e-01,  8.5137e-02,\n",
              "                                                         3.2450e-02, -2.2121e-01, -8.0889e-01, -5.7206e-02,  1.3427e-01,\n",
              "                                                        -3.2190e-02,  9.9999e-01, -1.4184e-01, -1.3690e-01,  1.4644e-01,\n",
              "                                                        -2.6983e-01, -1.7826e-01,  2.4423e-01, -2.3707e-01,  3.6375e-01,\n",
              "                                                        -2.6699e-01, -5.4524e-02, -2.4615e-01,  6.7699e-02, -2.5381e-01,\n",
              "                                                         2.7254e-01,  1.0355e-01, -1.3467e-01,  9.7743e-02, -2.7196e-01,\n",
              "                                                         9.0038e-02,  1.4215e-01,  1.5558e-01,  1.3703e-01,  2.3534e-01,\n",
              "                                                        -2.5741e-01, -1.3028e-01, -2.0318e-03, -1.0056e-01,  2.3707e-01,\n",
              "                                                        -3.5124e-01, -2.5083e-01,  6.5748e-02, -2.3660e-01, -2.3105e-02,\n",
              "                                                        -3.2675e-01,  1.5743e-01, -3.0924e-01,  2.4876e-01,  7.8535e-02,\n",
              "                                                        -2.4270e-01,  2.4242e-01,  2.6871e-01,  1.9806e-01,  2.2575e-01,\n",
              "                                                        -2.0163e-01,  8.1418e-01, -1.9489e-01,  2.1186e-01, -2.8169e-01,\n",
              "                                                        -1.1333e-01,  1.4982e-01,  2.7974e-01,  1.9777e-01, -2.0505e-01,\n",
              "                                                        -2.9175e-01,  2.1200e-01, -7.9246e-02,  1.5474e-01, -5.4317e-01,\n",
              "                                                         1.4153e-01, -4.0566e-01, -1.5925e-01,  1.1928e-01, -2.3556e-02,\n",
              "                                                         4.5854e-02, -2.6647e-01, -7.1401e-01, -5.5387e-02, -2.4340e-01,\n",
              "                                                         1.5323e-01,  1.7945e-01,  1.5285e-01,  2.4550e-01,  2.0081e-01,\n",
              "                                                        -1.9869e-02, -2.2867e-01, -2.0114e-01, -5.2901e-02,  1.0954e-01,\n",
              "                                                        -5.2247e-01,  1.4260e-01, -3.7692e-02, -1.9527e-01, -2.8998e-01,\n",
              "                                                         2.6884e-01, -2.0809e-01,  9.9999e-01,  1.5934e-02, -2.1602e-01,\n",
              "                                                         1.5476e-01, -1.4760e-01, -4.4385e-02, -1.3111e-01, -2.0028e-01,\n",
              "                                                         1.5662e-01,  1.7127e-01, -1.2768e-01,  1.9116e-01, -2.7243e-01,\n",
              "                                                        -2.4477e-01,  7.5690e-01,  2.9928e-01, -2.4304e-01, -7.6691e-02,\n",
              "                                                        -2.8176e-01,  1.3389e-02, -3.0895e-01,  2.9156e-01, -1.8190e-01,\n",
              "                                                         9.0539e-02,  1.6665e-01,  4.5462e-01, -2.4566e-01,  1.6648e-01,\n",
              "                                                        -3.3514e-01, -2.4303e-01, -2.3925e-01, -5.1180e-01, -2.0440e-01,\n",
              "                                                         1.1086e-01,  1.6733e-01, -1.5439e-01, -4.0740e-02,  2.7723e-01,\n",
              "                                                        -2.0302e-01,  1.2149e-01, -2.1369e-01, -2.8728e-01, -1.8188e-01,\n",
              "                                                         8.6878e-02, -8.6058e-02, -1.3908e-01,  3.3000e-01,  1.6924e-01,\n",
              "                                                         1.4972e-01, -1.1761e-01, -1.9778e-01,  6.8015e-02,  2.0811e-01,\n",
              "                                                         2.1961e-01,  6.7246e-02, -1.9363e-01, -1.3662e-01, -6.5474e-03,\n",
              "                                                        -2.7672e-01,  1.5963e-01,  6.3441e-02, -1.3053e-01,  1.3130e-01,\n",
              "                                                         4.3404e-01,  1.9479e-01, -1.0431e-01,  3.6378e-01, -4.8233e-01,\n",
              "                                                         1.4847e-01,  2.2081e-01, -2.5962e-01,  2.7266e-01,  1.7587e-01,\n",
              "                                                        -4.5138e-02, -1.4823e-01,  7.9432e-02, -2.1471e-01,  8.0764e-02,\n",
              "                                                         2.2730e-01,  1.0793e-01, -1.8070e-01,  1.6334e-01,  3.5029e-01,\n",
              "                                                         1.6779e-01, -2.1697e-01, -1.6407e-01,  1.2461e-01,  2.0277e-01,\n",
              "                                                        -2.6040e-01,  3.4569e-01,  1.7667e-01, -1.8150e-01, -1.9666e-01,\n",
              "                                                         1.5590e-01, -1.9236e-01, -1.9151e-01, -1.7022e-04,  2.1024e-01,\n",
              "                                                         2.8650e-01, -1.9690e-01, -9.7160e-02, -2.0327e-01, -1.0787e-01,\n",
              "                                                         9.7429e-02, -2.1399e-01, -3.0110e-01,  2.2920e-01, -3.8049e-01,\n",
              "                                                         1.5834e-01,  2.5018e-01, -3.2356e-01, -4.5978e-02, -1.6657e-01,\n",
              "                                                         1.7146e-01,  9.9999e-01, -2.6098e-01, -1.6209e-01, -9.9999e-01,\n",
              "                                                        -4.6891e-01, -2.7055e-01,  2.7301e-01,  1.6969e-01, -3.1661e-01,\n",
              "                                                        -7.0246e-02,  1.8259e-01, -1.8207e-01, -2.7109e-01,  3.0935e-01,\n",
              "                                                         4.0538e-01, -2.0407e-01,  1.3230e-01,  1.6996e-01,  5.7928e-02,\n",
              "                                                         1.4196e-01, -9.9999e-01, -1.0303e-01,  2.0970e-02, -4.2684e-02,\n",
              "                                                        -1.0149e-01, -2.2298e-01, -2.2930e-01, -1.1753e-01,  1.3490e-01,\n",
              "                                                         2.1198e-01,  2.2512e-01, -1.0939e-01, -2.5720e-01, -3.8533e-01,\n",
              "                                                         1.8747e-01, -1.2891e-01,  6.6848e-02, -2.2334e-01, -2.9430e-01,\n",
              "                                                         6.9859e-02,  2.0419e-01,  4.1753e-02,  2.8188e-01, -1.8594e-01,\n",
              "                                                         2.6414e-01, -2.4761e-01,  9.9999e-01,  1.9128e-01, -2.0234e-01,\n",
              "                                                         9.9999e-01,  2.6540e-01, -6.9420e-02, -5.2516e-01,  1.1561e-01,\n",
              "                                                         1.1352e-01,  9.9999e-01,  1.2856e-01, -3.0153e-01, -1.6637e-01,\n",
              "                                                         1.0502e-01,  2.0734e-01,  4.9180e-01, -4.1180e-03, -9.9999e-01,\n",
              "                                                         1.0031e-01,  1.9071e-01, -5.1545e-02, -6.5302e-02,  9.9999e-01,\n",
              "                                                        -9.6873e-02, -1.9128e-01,  1.1409e-01,  3.3795e-01,  1.3597e-01,\n",
              "                                                         3.7522e-01,  2.6428e-01, -9.0982e-02, -2.0914e-01, -1.1833e-01,\n",
              "                                                        -1.1229e-01, -3.4681e-01, -2.2249e-01,  1.0144e-01, -1.6956e-01,\n",
              "                                                        -1.1350e-01, -2.2901e-01,  3.6746e-02, -4.0213e-02,  9.9999e-01,\n",
              "                                                        -1.5811e-02,  1.7973e-01,  7.8818e-02,  1.9868e-01,  9.2021e-02,\n",
              "                                                        -7.2586e-02, -6.5820e-02, -3.6706e-01,  3.4459e-01, -2.6499e-01,\n",
              "                                                         1.7574e-01,  1.1026e-01,  5.2395e-02, -2.4073e-01, -1.0504e-01,\n",
              "                                                        -2.4620e-01,  2.0411e-01,  3.9539e-02,  5.3742e-01,  2.3484e-01,\n",
              "                                                        -1.6248e-01, -1.3831e-01,  2.2213e-01, -5.6143e-02,  2.6474e-01,\n",
              "                                                        -1.8035e-01,  9.9999e-01,  2.3786e-01, -1.9461e-01, -4.1616e-02,\n",
              "                                                        -1.0487e-01, -2.5266e-01, -1.6942e-01, -2.8441e-01,  5.9210e-01,\n",
              "                                                         1.1692e-01,  8.8994e-02,  1.4863e-01,  2.7514e-01, -8.7180e-02,\n",
              "                                                        -1.1124e-01,  1.6170e-01,  6.5175e-02,  4.0325e-02, -3.3775e-01,\n",
              "                                                        -9.9999e-01, -3.0970e-01,  1.5965e-01, -4.2635e-01, -2.5099e-01,\n",
              "                                                         1.5365e-03, -3.2543e-01, -1.5290e-01, -1.8129e-02,  7.3798e-02,\n",
              "                                                         5.0980e-02, -2.6010e-01, -2.0882e-01,  1.4205e-01, -1.4203e-01,\n",
              "                                                        -2.4697e-01,  1.9888e-01,  3.1164e-01,  3.2071e-01, -8.3327e-02,\n",
              "                                                         2.7376e-01,  1.7164e-01, -1.4848e-02,  2.0021e-01, -1.1181e-01,\n",
              "                                                         8.8273e-02, -3.5306e-01, -3.8330e-01, -1.9634e-01, -2.2946e-01,\n",
              "                                                         2.1131e-01, -2.8591e-01, -7.3601e-02,  1.2059e-01, -1.9083e-01,\n",
              "                                                         2.5192e-01,  1.9797e-01, -9.7534e-02, -1.8096e-01,  7.1655e-02,\n",
              "                                                        -7.0444e-02,  1.2615e-01, -3.8281e-01,  2.4260e-01, -1.2130e-02,\n",
              "                                                         1.8283e-01, -1.9107e-01, -3.1620e-03,  1.8127e-01, -2.2065e-01,\n",
              "                                                         1.7145e-01,  1.6610e-01,  2.1985e-01, -1.2444e-01, -2.6440e-01,\n",
              "                                                         3.6380e-02,  7.6215e-02,  2.8593e-01,  9.0590e-02, -1.8787e-01,\n",
              "                                                        -1.8267e-01, -2.0696e-01, -2.7640e-01, -1.0037e-02,  2.3924e-02,\n",
              "                                                        -9.9999e-01, -2.6962e-02, -2.0045e-01, -2.2580e-01,  2.4630e-01,\n",
              "                                                         9.2688e-02,  3.3989e-01, -2.0107e-02, -5.6714e-01,  2.8104e-01,\n",
              "                                                        -1.4707e-01, -2.9393e-01,  3.2152e-02,  1.3769e-01, -4.8022e-02,\n",
              "                                                         8.6200e-02, -9.9999e-01,  6.7207e-02,  2.5256e-01, -2.3375e-01,\n",
              "                                                         1.9723e-01,  1.8645e-01, -1.2572e-01,  1.3193e-01, -1.1801e-01,\n",
              "                                                         9.1986e-01,  9.8561e-02,  1.1744e-01, -1.2444e-01, -4.9978e-03,\n",
              "                                                        -2.2342e-01, -2.9113e-01, -1.8557e-01,  1.6803e-01, -2.7972e-02,\n",
              "                                                         1.7035e-01, -2.5341e-01,  2.5812e-01,  8.0004e-02,  4.3675e-01,\n",
              "                                                         2.2243e-01, -1.0360e-01,  8.2666e-02, -1.5962e-01,  1.8148e-01,\n",
              "                                                         1.2144e-01,  3.1488e-01, -1.0175e-01, -1.0262e-02,  1.7214e-01,\n",
              "                                                        -1.6515e-01,  1.9851e-01,  1.6968e-01,  9.3980e-02,  3.4524e-01,\n",
              "                                                         2.1478e-01,  2.7286e-01, -1.2090e-01, -1.3934e-01,  1.6480e-01,\n",
              "                                                         1.2709e-01, -3.1015e-01,  7.5589e-02,  1.4997e-01, -3.8406e-02,\n",
              "                                                        -4.3522e-02,  1.2363e-01, -1.9284e-01,  1.3526e-01, -2.3024e-01,\n",
              "                                                        -2.3264e-01, -1.8424e-01,  1.6026e-01, -1.1861e-01, -3.2132e-01,\n",
              "                                                         2.4170e-01, -1.9905e-01, -2.7806e-01, -1.4937e-01, -7.5356e-03,\n",
              "                                                         2.1491e-01, -2.6794e-01, -1.5013e-01,  1.8138e-01,  3.0317e-01,\n",
              "                                                         5.7694e-02,  2.1159e-01,  1.6390e-01, -1.2928e-01,  4.6134e-02,\n",
              "                                                         2.1114e-01, -2.5208e-01, -3.5166e-01,  2.0573e-01,  1.5310e-01,\n",
              "                                                         8.7928e-02,  1.9220e-01,  6.6093e-02, -2.9686e-01,  2.3148e-01,\n",
              "                                                         8.0450e-02, -9.1040e-02, -2.0886e-01,  1.3427e-01, -1.5787e-01,\n",
              "                                                        -1.6571e-01,  1.9406e-01, -9.9999e-01, -3.6244e-03, -8.9662e-02,\n",
              "                                                        -1.3008e-01,  3.1503e-01,  2.6301e-01,  2.0833e-01,  2.2039e-01,\n",
              "                                                        -3.5182e-02, -2.4805e-01, -2.8496e-01, -3.5234e-02, -4.7605e-02,\n",
              "                                                        -5.6250e-01,  1.1065e-01,  3.8267e-02, -1.0335e-01,  2.0949e-01,\n",
              "                                                        -4.6776e-01, -2.4236e-01, -1.9264e-01, -7.9509e-02,  2.5640e-01,\n",
              "                                                         1.3585e-01,  1.1146e-01, -4.2631e-01, -1.3797e-01, -4.2020e-02,\n",
              "                                                        -3.3589e-01, -3.2083e-01,  2.8107e-01,  2.4660e-01,  2.7439e-01,\n",
              "                                                        -6.5341e-01,  2.4811e-01, -2.2557e-01, -1.0728e-01,  1.3671e-01,\n",
              "                                                         1.7659e-01,  5.3338e-02, -1.9266e-01,  3.2107e-01, -1.6023e-01,\n",
              "                                                        -2.1777e-01,  1.0947e-01, -3.1034e-01,  1.6971e-01, -8.8409e-01,\n",
              "                                                         2.5713e-01,  1.5591e-01,  3.4103e-01, -2.2164e-01,  2.5962e-01,\n",
              "                                                         1.5191e-01,  1.2868e-01,  6.3957e-02,  2.0855e-01,  2.8089e-02,\n",
              "                                                         3.6860e-01, -1.4804e-01,  2.7331e-01,  4.2737e-01, -9.9999e-01,\n",
              "                                                        -1.2161e-01, -1.1271e-01, -2.3094e-01, -9.9999e-01, -1.4309e-01,\n",
              "                                                         1.6571e-01,  8.9037e-02,  2.0814e-01, -3.1244e-02,  2.2422e-01,\n",
              "                                                         3.3944e-01, -2.1243e-01, -2.2818e-01,  2.4313e-01, -2.5976e-01,\n",
              "                                                        -3.0441e-02,  2.5024e-01, -2.2388e-01, -1.9171e-01,  1.5171e-01,\n",
              "                                                         3.2445e-01,  4.3148e-01, -3.2404e-02, -5.4172e-01,  4.6540e-01,\n",
              "                                                        -1.2867e-01, -1.2872e-01, -4.5294e-01, -1.9522e-01,  2.7236e-01,\n",
              "                                                        -2.0159e-05,  1.6263e-01,  1.8271e-01,  2.9891e-01, -9.6923e-02,\n",
              "                                                        -2.4060e-01,  5.9646e-02, -1.2879e-01,  3.1317e-01,  1.3269e-01,\n",
              "                                                         1.5556e-01, -5.0566e-02,  5.3613e-01, -2.8093e-01, -1.4543e-01,\n",
              "                                                         3.8241e-02, -2.9764e-02,  4.5187e-01, -1.4331e-01, -2.3235e-01,\n",
              "                                                         3.9609e-01, -2.5692e-01, -1.0771e-01,  2.4073e-01,  1.2628e-01,\n",
              "                                                        -2.6284e-01, -1.0109e-01, -1.2280e-01,  3.4987e-01, -9.9999e-01,\n",
              "                                                        -8.0384e-02, -1.3164e-01,  2.1572e-01,  1.7950e-02, -1.7570e-01,\n",
              "                                                         8.7421e-01,  4.5491e-01,  8.7202e-02,  2.9448e-01, -3.2925e-01,\n",
              "                                                        -1.3678e-01, -3.7717e-01,  1.4726e-01,  1.3700e-01, -5.8430e-02,\n",
              "                                                         2.6105e-02, -1.6864e-01, -1.6733e-01]], grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "hi_sent = \"सोच मशीन\"\n",
        "tokenizer_output = mbert_tokenizer(hi_sent, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "mbert_model(input_ids, attention_mask = attn_mask)"
      ],
      "id": "6b0d2bf6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5877901b"
      },
      "source": [
        "Hence, we can very easily use mBERT for generating predictions on texts written in different languages."
      ],
      "id": "5877901b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b6f23d0"
      },
      "source": [
        "## Task 1: Fine-tune mBERT on XNLI\n",
        "\n",
        "We can now start fine-tuning mBERT on this dataset. We will start by defining the custom `Dataset` class for the task and then define the model and training loop."
      ],
      "id": "6b6f23d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45bf6240"
      },
      "source": [
        "## Task 1.1: Custom Dataset Class (2 Marks)\n",
        "\n",
        "Like in the previous assignments, implement the `XNLImBertDataset` class below that processes and stores the data as well as provides a way to iterate through the dataset. The details about various methods in the class are mentioned in their docstrings"
      ],
      "id": "45bf6240"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "984d8f79"
      },
      "outputs": [],
      "source": [
        "class XNLImBertDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, premises,\n",
        "                 hypotheses,\n",
        "                 labels,\n",
        "                 max_length,\n",
        "                mbert_variant = \"bert-base-multilingual-uncased\"):\n",
        "        \n",
        "        \"\"\"\n",
        "        Constructor for the `XNLImBertDataset` class. Stores the `premises`, `hypotheses` and `labels`\n",
        "        which can then be used by other methods. Also initializes the tokenizer.\n",
        "        \n",
        "        Inputs:\n",
        "            - premises (list) : A list of sentences constituting the premise in each example\n",
        "            - hypotheses (list) : A list of sentences constituting the hypothesis in each example\n",
        "            - labels (list) : A list of labels denoting for each premise-hypothesis pair.\n",
        "            - max_length (int): Maximum length of the encoded sequence.  \n",
        "                                If number of tokens are lower than `max_length` add padding otherwise truncate\n",
        "        \n",
        "        \n",
        "        Note that labels are in the form of strings \"entailment\", \"contradiction\" and \"neutral\". For training the\n",
        "        models we will want the labels in the numeric form, so you should define a mapping from the text label\n",
        "        to a numeric id. You should order the labels in alphabetical order while defining the mapping i.e. \n",
        "        contadiction -> 0, entailment -> 1, \"neutral\" - > 2 (such that we have consistency across everyone) \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        self.premises = None\n",
        "        self.hypotheses = None\n",
        "        self.labels = [0]*len(labels)\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = None\n",
        "        self.label2id = {'contradiction':0,'entailment':1,'neutral':2} # Define it as a dictionary\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        self.premises=premises\n",
        "        self.hypotheses=hypotheses\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(mbert_variant)\n",
        "        \n",
        "        for i in range(len(self.premises)):\n",
        "          self.labels[i]=self.label2id[labels[i]]\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset\n",
        "        \"\"\"\n",
        "        length = None\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        #raise NotImplementedError()\n",
        "        length=len( self.labels )\n",
        "        \n",
        "        return length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        \n",
        "        Returns the features and label corresponding to the the `idx` entry in the dataset.\n",
        "        \n",
        "        Inputs:\n",
        "            - idx (int): Index corresponding to the sentence_pair,label to be returned\n",
        "        \n",
        "        Returns:\n",
        "            - input_ids (torch.tensor): Indices of the tokens in the sentence pair.\n",
        "                                        Shape of the tensor should be (`seq_len`,)\n",
        "            - mask (torch.tensor): Attention mask indicating which tokens are padded.\n",
        "            - label (int): Label for the premise-hypothesis pair\n",
        "            \n",
        "        Hint: We have 2 sentences in a pair which must be concatenated using the [SEP] token before we tokenize and encode them\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        input_ids = None\n",
        "        mask = None\n",
        "        label = None\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        sentences=[0]*len(self.labels)\n",
        "        \n",
        "        for i in range(len(self.premises)):\n",
        "          sentences[i]=self.premises[i]+\"[SEP]\"+ self.hypotheses[i]\n",
        "\n",
        "        \n",
        "        tokenizer_output=self.tokenizer(sentences[idx],max_length=self.max_length, padding=\"max_length\", truncation = True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        mask=tokenizer_output[\"attention_mask\"]\n",
        "        input_ids=tokenizer_output[\"input_ids\"]\n",
        "        label=self.labels[idx]\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        return input_ids.squeeze(0), mask.squeeze(0), label"
      ],
      "id": "984d8f79"
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4e1ba52b",
        "outputId": "39566a5c-3247-47eb-ab65-f996b168fec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1: Checking if `__len__` is implemented correctly\n",
            "Dataset Length: 3\n",
            "Expected Length: 3\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\n",
            "input_ids:\n",
            " tensor([  101,   143, 10564, 15450, 84789, 10107, 10103, 38884, 10108,   143,\n",
            "        16745, 10104, 10970, 11344, 17147, 11913,   119,   102, 10103, 10564,\n",
            "        10127, 55860,   119,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "Expected input_ids:\n",
            " tensor([  101,   143, 10564, 15450, 84789, 10107, 10103, 38884, 10108,   143,\n",
            "        16745, 10104, 10970, 11344, 17147, 11913,   119,   102, 10103, 10564,\n",
            "        10127, 55860,   119,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "label:\n",
            " 0\n",
            "Expected label:\n",
            " 0\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\n",
            "input_ids:\n",
            " tensor([  101, 10144, 18585, 10110, 24392, 10564, 14965, 64581,   119,   102,\n",
            "        10536, 10562, 10320, 14965, 64581, 10110, 18418, 82863, 10160, 10103,\n",
            "        45670, 14734, 10125, 10103, 21005,   119,   102,     0,     0,     0,\n",
            "            0,     0])\n",
            "Expected input_ids:\n",
            " tensor([  101, 10144, 18585, 10110, 24392, 10564, 14965, 64581,   119,   102,\n",
            "        10536, 10562, 10320, 14965, 64581, 10110, 18418, 82863, 10160, 10103,\n",
            "        45670, 14734, 10125, 10103, 21005,   119,   102,     0,     0,     0,\n",
            "            0,     0])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0])\n",
            "label:\n",
            " 2\n",
            "Expected label:\n",
            " 2\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 2`\n",
            "input_ids:\n",
            " tensor([  101,   143, 20071, 11336, 10171, 18248, 19592, 14734,   119,   102,\n",
            "        10970, 10562, 10320, 14734,   143, 13148,   119,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "Expected input_ids:\n",
            " tensor([  101,   143, 20071, 11336, 10171, 18248, 19592, 14734,   119,   102,\n",
            "        10970, 10562, 10320, 14734,   143, 13148,   119,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "label:\n",
            " 1\n",
            "Expected label:\n",
            " 1\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 5: Checking for hindi\n",
            "input_ids:\n",
            " tensor([  101, 11384,   569, 30119, 10949, 11142, 74535, 10949,   533, 13764,\n",
            "        25695,   571, 12114, 19086, 10949, 36335,   580,   591,   102,   568,\n",
            "        11551, 17109, 12334, 56426, 52061,   569, 28393, 41790, 20106, 11483,\n",
            "        91329, 19086, 29931,   533, 13764,   102])\n",
            "Expected input_ids:\n",
            " tensor([  101, 11384,   569, 30119, 10949, 11142, 74535, 10949,   533, 13764,\n",
            "        25695,   571, 12114, 19086, 10949, 36335,   580,   591,   102,   568,\n",
            "        11551, 17109, 12334, 56426, 52061,   569, 28393, 41790, 20106, 11483,\n",
            "        91329, 19086, 29931,   533, 13764,   102])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "label:\n",
            " 2\n",
            "Expected label:\n",
            " 2\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Cases\")\n",
        "sample_premises = [\"A man inspects the uniform of a figure in some East Asian country.\",\n",
        "                    \"An older and younger man smiling.\",\n",
        "                   \"A soccer game with multiple males playing.\"\n",
        "                    ]\n",
        "sample_hypotheses = [\"The man is sleeping.\",\n",
        "                     \"Two men are smiling and laughing at the cats playing on the floor.\",\n",
        "                    \"Some men are playing a sport.\"]\n",
        "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
        "sample_max_len = 32\n",
        "sample_dataset = XNLImBertDataset(\n",
        "    sample_premises,\n",
        "    sample_hypotheses,\n",
        "    sample_labels,\n",
        "    sample_max_len\n",
        ")\n",
        "print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n",
        "dataset_len= len(sample_dataset)\n",
        "expected_len = len(sample_labels)\n",
        "print(f\"Dataset Length: {dataset_len}\")\n",
        "print(f\"Expected Length: {expected_len}\")\n",
        "assert len(sample_dataset) == len(sample_premises)\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n",
        "sample_idx = 0\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids =  torch.tensor([  101,   143, 10564, 15450, 84789, 10107, 10103, 38884, 10108,   143,\n",
        "        16745, 10104, 10970, 11344, 17147, 11913,   119,   102, 10103, 10564,\n",
        "        10127, 55860,   119,   102,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0])\n",
        "expected_label = 0\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n",
        "sample_idx = 1\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids = torch.tensor([  101, 10144, 18585, 10110, 24392, 10564, 14965, 64581,   119,   102,\n",
        "        10536, 10562, 10320, 14965, 64581, 10110, 18418, 82863, 10160, 10103,\n",
        "        45670, 14734, 10125, 10103, 21005,   119,   102,     0,     0,     0,\n",
        "            0,     0])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 0, 0, 0, 0, 0])\n",
        "expected_label = 2\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "\n",
        "print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 2`\")\n",
        "sample_idx = 2\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids = torch.tensor([  101,   143, 20071, 11336, 10171, 18248, 19592, 14734,   119,   102,\n",
        "        10970, 10562, 10320, 14734,   143, 13148,   119,   102,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0])\n",
        "expected_label = 1\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "\n",
        "\n",
        "sample_premises = [\"एक आदमी किसी पूर्वी एशियाई देश में एक आकृति की वर्दी का निरीक्षण करता है।\",\n",
        "                    \"एक बूढ़ा और छोटा आदमी मुस्कुरा रहा है।\",\n",
        "                   \"एक फ़ुटबॉल खेल जिसमें कई पुरुष खेल रहे हैं।\"\n",
        "                    ]\n",
        "sample_sentence2s = [\"आदमी सो रहा है।\",\n",
        "                     \"फर्श पर खेल रही बिल्लियों को देखकर दो आदमी मुस्कुरा रहे हैं और हंस रहे हैं।\",\n",
        "                    \"कुछ पुरुष कोई खेल खेल रहे हैं।\"\n",
        "                    ]\n",
        "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
        "sample_max_len = 36\n",
        "sample_dataset = XNLImBertDataset(\n",
        "    sample_premises,\n",
        "    sample_sentence2s,\n",
        "    sample_labels,\n",
        "    sample_max_len\n",
        ")\n",
        "\n",
        "print(f\"Sample Test Case 5: Checking for hindi\")\n",
        "sample_idx = 1\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids =  torch.tensor([  101, 11384,   569, 30119, 10949, 11142, 74535, 10949,   533, 13764,\n",
        "        25695,   571, 12114, 19086, 10949, 36335,   580,   591,   102,   568,\n",
        "        11551, 17109, 12334, 56426, 52061,   569, 28393, 41790, 20106, 11483,\n",
        "        91329, 19086, 29931,   533, 13764,   102])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "expected_label = 2\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "\n"
      ],
      "id": "4e1ba52b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3863fe4d"
      },
      "source": [
        "Initialize dataset and dataloaders for english training and validation sets"
      ],
      "id": "3863fe4d"
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "80913533"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 128\n",
        "batch_size = 8\n",
        "\n",
        "train_en_premises, train_en_hypotheses = train_en_data[\"premise\"].values, train_en_data[\"hypothesis\"].values\n",
        "train_en_labels = train_en_data[\"label\"].values\n",
        "\n",
        "val_en_premises, val_en_hypotheses = val_en_data[\"premise\"].values, val_en_data[\"hypothesis\"].values\n",
        "val_en_labels = val_en_data[\"label\"].values\n",
        "\n",
        "train_en_dataset = XNLImBertDataset(train_en_premises, train_en_hypotheses, train_en_labels, max_seq_len)\n",
        "val_en_dataset = XNLImBertDataset(val_en_premises, val_en_hypotheses, val_en_labels, max_seq_len)\n",
        "\n",
        "train_en_dataloader = DataLoader(train_en_dataset, batch_size = batch_size)\n",
        "val_en_dataloader = DataLoader(val_en_dataset, batch_size = batch_size)"
      ],
      "id": "80913533"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ca63c08"
      },
      "source": [
        "## Task 1.2: Implement mBERT Based Classifier for NLI (2 Marks)\n",
        "\n",
        "Similar to last assignment implement a classifier with an mBERT module followed by a classification layer. Note that unlike last time we have 3 classes now, so we can no longer use Sigmoid function in the output layer and instead will need to use the Softmax function. You can refer [here](https://cs231n.github.io/linear-classify/#softmax-classifier) if you need a primer on how the softmax function works. Hence, this time instead of getting a single output from the model for an input, denoting the probability of the poistive class, we will get 3 numbers as output for each input denoting the probability of each of the 3 classes. Also, it is common to use Log of the Softmax function instead of plain softmax to obtain log-probabilities. Log-Softmax is numerically more stable and hence it is often more used in practice. You can read about it's usage in pytorch [here](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html). Implement the `mBERTNLIClassifierModel` below"
      ],
      "id": "5ca63c08"
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "874c8194"
      },
      "outputs": [],
      "source": [
        "\n",
        "class mBERTNLIClassifierModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_hidden = 768, mbert_variant = \"bert-base-multilingual-uncased\"):\n",
        "        \n",
        "        \"\"\"\n",
        "        Constructor for the `mBERTNLIClassifierModel` class. Use this to define  the network architecture\n",
        "        which should be: Input -> mBERT -> Linear Layer -> Log-Softmax\n",
        "        \n",
        "        Inputs:\n",
        "            - d_hidden (int): Size of the hidden representations of mbert\n",
        "            - mbert_variant (str): mBERT variant to use\n",
        "        \n",
        "        \"\"\"\n",
        "        super(mBERTNLIClassifierModel, self).__init__()\n",
        "        \n",
        "        self.mbert_layer = None\n",
        "        self.output_layer = None\n",
        "        self.log_softmax_layer = None\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        self.mbert_layer=BertModel.from_pretrained(mbert_variant)\n",
        "        self.output_layer= nn.Linear(d_hidden, 3)\n",
        "        self.log_softmax_layer = nn.LogSoftmax(dim=1)\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids, attn_mask):\n",
        "        \n",
        "        \"\"\"\n",
        "        Forward Passes the inputs through the network and obtains the prediction\n",
        "        \n",
        "        Inputs:\n",
        "            - input_ids (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
        "                                        representing the sequence of token ids\n",
        "            - attn_mask (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
        "                                        representing the attention mask such that padded tokens are 0 and rest 1\n",
        "                                        \n",
        "        Returns:\n",
        "          - output (torch.tensor): A torch tensor of shape [batch_size, 3] containing (log) probabilities\n",
        "          of each class \n",
        "                                                \n",
        "        \"\"\"\n",
        "        \n",
        "        output = None\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        o=  self.mbert_layer(input_ids, attention_mask = attn_mask)\n",
        "  \n",
        "        output = self.log_softmax_layer (self.output_layer(o.pooler_output))\n",
        "        #raise NotImplementedError()\n",
        "        \n",
        "        return output"
      ],
      "id": "874c8194"
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "06229181",
        "outputId": "4e9ed777-00e5-49ca-e34b-ea5bc31a128c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Test Case 1\n",
            "Model Output: [[-0.9885042 -1.479876  -0.9157878]]\n",
            "Expected Output: [[-0.9885041 -1.479876  -0.915788 ]]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n",
            "Sample Test Case 2\n",
            "Model Output: [[-0.9744187  -1.4775381  -0.93041617]]\n",
            "Expected Output: [[-0.97441876 -1.4775381  -0.9304163 ]]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Running Sample Test Cases!\")\n",
        "torch.manual_seed(42)\n",
        "model = mBERTNLIClassifierModel()\n",
        "\n",
        "sample_premises = [\"A man inspects the uniform of a figure in some East Asian country.\",\n",
        "                    \"An older and younger man smiling.\",\n",
        "                   \"A soccer game with multiple males playing.\"\n",
        "                    ]\n",
        "sample_hypotheses = [\"The man is sleeping.\",\n",
        "                     \"Two men are smiling and laughing at the cats playing on the floor.\",\n",
        "                    \"Some men are playing a sport.\"]\n",
        "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
        "sample_max_len = 32\n",
        "sample_dataset = XNLImBertDataset(\n",
        "    sample_premises,\n",
        "    sample_hypotheses,\n",
        "    sample_labels,\n",
        "    sample_max_len\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Sample Test Case 1\")\n",
        "sample_idx = 0\n",
        "input_ids, attn_mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "mbert_cls_out = model(input_ids.unsqueeze(0), attn_mask.unsqueeze(0)).detach().numpy()\n",
        "expected_mbert_cls_out = np.array([[-0.9885041, -1.479876,  -0.915788 ]])\n",
        "print(f\"Model Output: {mbert_cls_out }\")\n",
        "print(f\"Expected Output: {expected_mbert_cls_out}\")\n",
        "\n",
        "assert mbert_cls_out .shape == expected_mbert_cls_out.shape\n",
        "assert np.allclose(mbert_cls_out, expected_mbert_cls_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n",
        "\n",
        "print(\"Sample Test Case 2\")\n",
        "sample_idx = 1\n",
        "input_ids, attn_mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "mbert_cls_out = model(input_ids.unsqueeze(0), attn_mask.unsqueeze(0)).detach().numpy()\n",
        "expected_mbert_cls_out = np.array([[-0.97441876, -1.4775381,  -0.9304163 ]])\n",
        "print(f\"Model Output: {mbert_cls_out }\")\n",
        "print(f\"Expected Output: {expected_mbert_cls_out}\")\n",
        "\n",
        "assert mbert_cls_out .shape == expected_mbert_cls_out.shape\n",
        "assert np.allclose(mbert_cls_out, expected_mbert_cls_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n"
      ],
      "id": "06229181"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30c8ca8f"
      },
      "source": [
        "## Task 1.3: Training and Evaluating the Model (4 Marks)\n",
        "\n",
        "Similar to previous assignments implement the `train` and `evaluate` functions below. There will be though a couple of differences this time. First, for training the model we can no longer use Binary Cross Entropy Loss because as the name suggests it is applicable for binary classification problems. Instead we will use the [Negative Log-Likelihood Loss function](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) instead. Second, while evaluating the accuracy we can no longer use a threshold to convert the probabilities into the labels, since we will now have 3 probability values instead of a single one, corresponding to the each class. In such cases it is common to predict the class as the label which has the highest probability (or equivalently log probability).\n"
      ],
      "id": "30c8ca8f"
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "FpxgK-5scVN-"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(pred_labels, act_labels):\n",
        "  l = len(pred_labels)\n",
        "  #print(l)\n",
        "  lst= []\n",
        " \n",
        "  for i in range(l):\n",
        "    if (pred_labels[i]==act_labels[i]):\n",
        "      lst.append(1)\n",
        "    else:\n",
        "      lst.append(0)\n",
        "\n",
        "  accuracy = sum(lst)/l\n",
        "  \n",
        "  # YOUR CODE HERE\n",
        "  #raise NotImplementedError()\n",
        "\n",
        "  return accuracy"
      ],
      "id": "FpxgK-5scVN-"
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "TUSLvDMMcZm9"
      },
      "outputs": [],
      "source": [
        "def convert_probs_to_labels(pred_probs):\n",
        "  \n",
        "  labels=torch.argmax(torch.tensor(pred_probs), dim=1)\n",
        "  # YOUR CODE HERE\n",
        "  #raise NotImplementedError()\n",
        "\n",
        "  return labels"
      ],
      "id": "TUSLvDMMcZm9"
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "787c97cf"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataloader,device=\"cuda\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluates `model` on test dataset\n",
        "\n",
        "    Inputs:\n",
        "        - model (mBERTNLIClassifierModel): mBERT based classifier model to be evaluated\n",
        "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
        "\n",
        "    Returns:\n",
        "        - accuracy (float): Average accuracy over the test dataset \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    accuracy = None\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "   \n",
        "    accuracy = 0\n",
        "    #raise NotImplementedError()\n",
        "    with torch.no_grad():\n",
        "      for test_batch in test_dataloader:\n",
        "        input_ids, mask, labels = test_batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        mask=mask.to(device)\n",
        "        #labels =labels.to(device)\n",
        "        #print(labels)\n",
        "        pred_probs = model(input_ids, mask)\n",
        "        pred_probs=pred_probs\n",
        "        #print(pred_probs)\n",
        "        #pred_probs=pred_probs.to(device)\n",
        "        pred_probs = pred_probs.detach().cpu().numpy()\n",
        "        input_ids =  input_ids.detach().cpu().numpy()\n",
        "        batch_accuracy = get_accuracy(convert_probs_to_labels(pred_probs), labels)\n",
        "        accuracy+=batch_accuracy\n",
        "\n",
        "    \n",
        "    return accuracy/len(test_dataloader)\n",
        "    \n",
        "    \n",
        "    \n",
        "def train(model, train_dataloader, val_dataloader,\n",
        "          lr = 1e-5, num_epochs = 3,device=\"cuda\"\n",
        "          ):\n",
        "    \n",
        "    \"\"\"\n",
        "    Runs the training loop. Define the loss function as NLLLoss\n",
        "    and optimizer as Adam and train for `num_epochs` epochs.\n",
        "\n",
        "    Inputs:\n",
        "        - model (mBERTNLIClassifierModel): mBERT based classifer model to be trained\n",
        "        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
        "        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n",
        "        - lr (float): The learning rate for the optimizer\n",
        "        - num_epochs (int): Number of epochs to train the model for.\n",
        "        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        - best_model (mBERTNLIClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n",
        "        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "        \n",
        "    best_val_accuracy = float(\"-inf\")\n",
        "    best_model = None\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    loss_fn = torch.nn.NLLLoss()\n",
        "    optimizer = Adam(model.parameters(), lr)\n",
        "    #raise NotImplementedError()\n",
        "    for epoch in range(num_epochs):\n",
        "        #model.train() # Since we are evaluating model at the end of every epoch, it is important to bring it back to train mode\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        # 2. Write Training Loop (store the loss for each batch in epoch_loss like done in previous assignments)\n",
        "        # YOUR CODE HERE\n",
        "        for train_batch in tqdm.tqdm(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            #print(train_batch)\n",
        "            input_ids, mask, labels= train_batch\n",
        "            \n",
        "            input_ids = input_ids.to(device)\n",
        "            mask=mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            preds = model(input_ids, mask)\n",
        "            \n",
        "            loss = loss_fn(preds, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "       \n",
        "        \n",
        "        epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        \n",
        "        # 3. Evaluate on validation data by calling `evaluate` and store the validation accuracy in `val_accurracy`\n",
        "        val_accuracy = 0\n",
        "        # YOUR CODE HERE\n",
        "        val_accuracy =evaluate(model, val_dataloader)\n",
        "        \n",
        "        \n",
        "        # Model selection\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model = copy.deepcopy(model) # Create a copy of model\n",
        "        \n",
        "        print(f\"Epoch {epoch} completed | Average Training Loss: {epoch_loss} | Validation Accuracy: {val_accuracy}\")\n",
        " \n",
        "    best_model.zero_grad()\n",
        "    return best_model, best_val_accuracy"
      ],
      "id": "787c97cf"
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5c77e27c",
        "outputId": "940f1cb8-5de0-4c9e-a548-a7692520e3ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 100 data points for sanity check\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 13/13 [00:01<00:00,  7.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed | Average Training Loss: 1.1184610861998339 | Validation Accuracy: 0.375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  6.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed | Average Training Loss: 1.089647889137268 | Validation Accuracy: 0.5480769230769231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed | Average Training Loss: 1.0361034915997431 | Validation Accuracy: 0.7211538461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed | Average Training Loss: 0.7213530563391172 | Validation Accuracy: 0.7884615384615384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed | Average Training Loss: 0.34705049659197146 | Validation Accuracy: 0.9903846153846154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed | Average Training Loss: 0.10797872623571983 | Validation Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed | Average Training Loss: 0.20192756195767567 | Validation Accuracy: 0.9807692307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed | Average Training Loss: 0.16816660331992003 | Validation Accuracy: 0.9903846153846154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed | Average Training Loss: 0.03179901849048642 | Validation Accuracy: 0.9807692307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:01<00:00,  7.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 completed | Average Training Loss: 0.08652137474228556 | Validation Accuracy: 1.0\n",
            "Best Validation Accuracy: 1.0\n",
            "Expected Best Validation Accuracy: 0.99\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "print(\"Training on 100 data points for sanity check\")\n",
        "\n",
        "max_seq_len = 128\n",
        "batch_size = 8\n",
        "\n",
        "sample_premises, sample_hypotheses = train_en_data[\"premise\"].values[:100], train_en_data[\"hypothesis\"].values[:100]\n",
        "sample_labels = train_en_data[\"label\"].values[:100]\n",
        "\n",
        "sample_dataset = XNLImBertDataset(sample_premises, sample_hypotheses, sample_labels, max_seq_len)\n",
        "sample_dataloader = DataLoader(sample_dataset, batch_size = batch_size)\n",
        "\n",
        "\n",
        "model = mBERTNLIClassifierModel()\n",
        "best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, lr = 5e-5, num_epochs = 10, device=\"cuda\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
        "print(f\"Expected Best Validation Accuracy: {0.99}\")"
      ],
      "id": "5c77e27c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a628b8d9"
      },
      "source": [
        "Since we just trained and evaluated on same 100 examples, you should expect nearly perfect 99% accuracy. Now let's train on the entire dataset."
      ],
      "id": "a628b8d9"
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "a8d168c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d34b9df8-2024-4783-e36a-9f5680e0a577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 4750/4750 [27:21<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed | Average Training Loss: 0.7354497155578513 | Validation Accuracy: 0.7185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4750/4750 [27:10<00:00,  2.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed | Average Training Loss: 0.41100476534645025 | Validation Accuracy: 0.725\n",
            "Best Validation Accuracy: 0.725\n",
            "Expected Best Validation Accuracy: 0.7675\n"
          ]
        }
      ],
      "source": [
        "model = mBERTNLIClassifierModel()\n",
        "best_model, best_val_acc = train(model, train_en_dataloader, val_en_dataloader, lr = 1e-5, num_epochs = 2)\n",
        "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
        "print(f\"Expected Best Validation Accuracy: {0.7675}\")"
      ],
      "id": "a8d168c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e700e982"
      },
      "source": [
        "## Task 1.4: Zero-Shot Transfer (2 Marks)\n",
        "\n",
        "Pre-trained multilingual models like mBERT have shown to exhibit zero-shot transfer capabilities to new langauges for which the model was never fine-tuned on. You can read more about zero-shot transfer in mBERT in this [paper](https://arxiv.org/abs/1906.01502). We now test this phenomenon for ourselves, where we will evaluate the performance of the mBERT classifier that we just trained on the English on the test sets in 15 different languages. Implement the `evaluate_on_diff_langs` function below that does that"
      ],
      "id": "e700e982"
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "7dcad1a6"
      },
      "outputs": [],
      "source": [
        "def evaluate_on_diff_langs(model, lang2test_df, max_length = 128, batch_size = 8, device = \"cuda\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of the fine-tuned model on test data in different langauges.\n",
        "    \n",
        "    Inputs:\n",
        "        - model (mBERTNLIClassifierModel): mBERT based classifer model fine-tuned on English data\n",
        "        - lang2test_df (dict): A dictionary with langauges as keys and\n",
        "                                their corresponding test sets (in form of pandas dataframe)\n",
        "                                as values\n",
        "                                \n",
        "    Returns:\n",
        "        - lang2acc (dict): A dictionary with language ids as keys and the accuracy on it's test set as values\n",
        "                            eg: {\"en\" : 0.8, \"fr\" : 0.77, \"hi\": 0.72, ...}\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    lang2acc = {}\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "   \n",
        "    language_values=list(lang2test_df.values())\n",
        "  \n",
        "    language=list(lang2test_df.keys())\n",
        "    \n",
        "   \n",
        "    for i in range(len(language_values)):\n",
        "      test_data=language_values[i]\n",
        "      test_premises, test_hypotheses = test_data[\"premise\"].values, test_data[\"hypothesis\"].values\n",
        "      test_labels = test_data[\"label\"].values  \n",
        "      test_dataset = XNLImBertDataset(test_premises, test_hypotheses, test_labels, max_length)\n",
        "    \n",
        "      test_dataloader = DataLoader( test_dataset, batch_size = batch_size)\n",
        "      #lang_acc=evaluate(model, test_dataloader, device = \"cuda\")\n",
        "      accuracy=evaluate(model, test_dataloader,device=\"cuda\")\n",
        "      \n",
        "      lang2acc[language[i]]=accuracy\n",
        "    return lang2acc  \n",
        "   \n",
        "\n",
        "    \n",
        "    \n",
        "    #lang2acc = evaluate(model, test_en_dataloader, device = \"cpu\")\n",
        "\n",
        "    #raise NotImplementedError()\n",
        "    \n",
        "    "
      ],
      "id": "7dcad1a6"
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "39ff6b5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b0efbf-d597-45c8-b5d0-c1c8841851b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langauge to Accuracy:\n",
            " {'ar': 0.5665064102564102, 'bg': 0.610176282051282, 'de': 0.6538461538461539, 'el': 0.5913461538461539, 'en': 0.7139423076923077, 'es': 0.6658653846153846, 'fr': 0.6510416666666666, 'hi': 0.5480769230769231, 'ru': 0.6105769230769231, 'sw': 0.5132211538461539, 'th': 0.35096153846153844, 'tr': 0.5981570512820513, 'ur': 0.5356570512820513, 'vi': 0.5933493589743589, 'zh': 0.5805288461538461}\n",
            "Expected Values:\n",
            " {'ar': 0.5989583333333334, 'bg': 0.6454326923076923, 'de': 0.6698717948717948, 'el': 0.6402243589743589, 'en': 0.7263621794871795, 'es': 0.6923076923076923, 'fr': 0.6802884615384616, 'hi': 0.5893429487179487, 'ru': 0.6478365384615384, 'sw': 0.53125, 'th': 0.35136217948717946, 'tr': 0.610176282051282, 'ur': 0.5637019230769231, 'vi': 0.6193910256410257, 'zh': 0.6073717948717948}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lang2acc = evaluate_on_diff_langs(best_model, lang2test_df, max_length = 128, batch_size = 8, device = \"cuda\")\n",
        "expected_vals = {'ar': 0.5989583333333334,\n",
        " 'bg': 0.6454326923076923,\n",
        " 'de': 0.6698717948717948,\n",
        " 'el': 0.6402243589743589,\n",
        " 'en': 0.7263621794871795,\n",
        " 'es': 0.6923076923076923,\n",
        " 'fr': 0.6802884615384616,\n",
        " 'hi': 0.5893429487179487,\n",
        " 'ru': 0.6478365384615384,\n",
        " 'sw': 0.53125,\n",
        " 'th': 0.35136217948717946,\n",
        " 'tr': 0.610176282051282,\n",
        " 'ur': 0.5637019230769231,\n",
        " 'vi': 0.6193910256410257,\n",
        " 'zh': 0.6073717948717948}\n",
        "print(f\"Langauge to Accuracy:\\n {lang2acc}\")\n",
        "print(f\"Expected Values:\\n {expected_vals}\")"
      ],
      "id": "39ff6b5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65558d2f"
      },
      "source": [
        "Don't worry if the values do not match exactly, but you can expect similar patterns i.e. the fine-tuned model on English data, performs reasonably on other new langauges as well compared to it's performance on English test data. Performance on langauges like German, French and Spanish is much closer to the performance on English. However, it is on the lower side for languages like Swahilli, Urdu and Thai. The values are still surprisingly high, considering a random guess will fetch you an accuracy of 33%."
      ],
      "id": "65558d2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "318c3169"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "318c3169"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6b6f23d0"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}